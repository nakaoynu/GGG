# unified_weighted_bayesian_fitting.py - ç£å ´ãƒ»æ¸©åº¦ä¸€æ‹¬ãƒ™ã‚¤ã‚ºæ¨å®šãƒ—ãƒ­ã‚°ãƒ©ãƒ 
#
# ã€æ¦‚è¦ã€‘
# ã“ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¯ã€ä»¥ä¸‹ã®2ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¸€æ‹¬å‡¦ç†ã—ã¦ãƒ™ã‚¤ã‚ºæ¨å®šã‚’å®Ÿè¡Œã—ã¾ã™:
#   â‘  æ¸©åº¦å¤‰æ•°ã€ç£å ´å›ºå®š: [B, T] = [9.0, 4], [9.0, 10], [9.0, 20], ...
#   â‘¡ æ¸©åº¦å›ºå®šã€ç£å ´å¤‰æ•°: [B, T] = [1.0, 4], [2.0, 4], [3.0, 4], ...
#
# ã€ç‰¹å¾´ã€‘
# - weighted_bayesian_fitting_completed.pyã¨åŒæ§˜ã®ç‰©ç†ãƒ¢ãƒ‡ãƒ«ãƒ»æ‰‹æ³•ã‚’ä½¿ç”¨
# - Î³ã¯ç·šå½¢ã®æ¸©åº¦ä¾å­˜æ€§ã‚’ç¤ºã™è¨­å®š
# - Îµ_bgã¯ãã‚Œãã‚Œã®ç£å ´ãƒ»æ¸©åº¦ã§ç‹¬ç«‹ã«éç·šå½¢æœ€å°äºŒä¹—æ³•ã§æ±ºå®š
# - ãã®ä»–ã®ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯é‡ã¿ä»˜ããƒ™ã‚¤ã‚ºæ¨å®šã§ä¸€æ‹¬æ±ºå®š
# - H_formã¨B_formã®ä¸¡ãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œ
#
# ã€ãƒ‡ãƒ¼ã‚¿è¦ä»¶ã€‘
# Excelãƒ•ã‚¡ã‚¤ãƒ«ã«ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦:
# - æ¸©åº¦å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿: å„æ¸©åº¦åˆ— (ä¾‹: '4K', '10K', '20K', ...)
# - ç£å ´å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿: å„ç£å ´åˆ— (ä¾‹: '1T', '2T', '3T', ...)

# ============================================================================
# ã€é‡è¦ã€‘CPUä¸¦åˆ—åŒ–ã®ç’°å¢ƒå¤‰æ•°è¨­å®š
# NumPy/SciPyã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹
# ============================================================================
import os
import pathlib

def _setup_cpu_threads():
    """NumPyã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰ã«CPUä¸¦åˆ—ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’è¨­å®š"""
    try:
        # YAMLã ã‘å…ˆã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆè»½é‡ï¼‰
        import yaml
        config_path = pathlib.Path(__file__).parent / "config_unified.yml"
        with open(config_path, 'r', encoding='utf-8') as f:
            temp_config = yaml.safe_load(f)
        
        exec_config = temp_config.get('execution', {})
        threads_per_chain = exec_config.get('threads_per_chain', None)
        
        if threads_per_chain is not None:
            threads_str = str(threads_per_chain)
            # å…¨ã¦ã®ä¸¦åˆ—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«è¨­å®š
            os.environ['OMP_NUM_THREADS'] = threads_str
            os.environ['MKL_NUM_THREADS'] = threads_str
            os.environ['OPENBLAS_NUM_THREADS'] = threads_str
            os.environ['NUMEXPR_NUM_THREADS'] = threads_str
            os.environ['VECLIB_MAXIMUM_THREADS'] = threads_str
            
            mcmc_config = temp_config.get('mcmc', {})
            chains = mcmc_config.get('chains', 4)
            total_threads = chains * threads_per_chain
            print(f"âš¡ CPUä¸¦åˆ—è¨­å®š: {threads_str} threads/chain Ã— {chains} chains = {total_threads} vCPUs")
        else:
            print("â„¹ï¸  threads_per_chainæœªè¨­å®š: ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨")
            
    except Exception as e:
        print(f"âš ï¸ CPUä¸¦åˆ—è¨­å®šå¤±æ•—: {e}")

# NumPyã‚ˆã‚Šå…ˆã«ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
_setup_cpu_threads()

# PyTensorè¨­å®šï¼ˆCPUå°‚ç”¨ï¼‰
os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float64'

# ============================================================================

print("="*70)
print("ç£å ´ãƒ»æ¸©åº¦ä¸€æ‹¬ãƒ™ã‚¤ã‚ºæ¨å®šãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
print("="*70)
print("\nâ³ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’èª­ã¿è¾¼ã¿ä¸­... (åˆå›ã¯2-5åˆ†ç¨‹åº¦ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™)")
print("   Ctrl+Cã§ä¸­æ–­ã—ãªã„ã§ãã ã•ã„\n")

import time
_import_start = time.time()

import datetime
import warnings
import yaml
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import arviz as az
import pymc as pm
import pytensor.tensor as pt
from pytensor.graph.op import Op
from typing import List, Dict, Any, Tuple, Optional, Union
from scipy.signal import find_peaks, peak_widths
from scipy.optimize import curve_fit
_import_time = time.time() - _import_start
print(f"\nâœ… å…¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿å®Œäº†! (æ‰€è¦æ™‚é–“: {_import_time:.1f}ç§’)\n")

# æ•°å€¤è¨ˆç®—ã®è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore', category=RuntimeWarning)
np.seterr(all='ignore')

try:
    import japanize_matplotlib
except ImportError:
    print("è­¦å‘Š: japanize_matplotlib ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

plt.rcParams['figure.dpi'] = 120

# --- ç‰©ç†å®šæ•° ---
kB = 1.380649e-23
muB = 9.274010e-24
hbar = 1.054571e-34
c = 299792458
mu0 = 4.0 * np.pi * 1e-7
s = 3.5  # ã‚¹ãƒ”ãƒ³é‡å­æ•° 

# --- THzå˜ä½ç³»å¤‰æ›å®šæ•° ---
# æ•°å€¤è¨ˆç®—ã®å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã€å‘¨æ³¢æ•°ãƒ»ç·©å’Œç‡ã‚’THzå˜ä½ã§æ‰±ã†
# 1 THz = 10^12 Hz = 2Ï€ Ã— 10^12 rad/s
THZ_TO_RAD_S = 2.0 * np.pi * 1e12  # THz â†’ rad/s å¤‰æ›ä¿‚æ•°
RAD_S_TO_THZ = 1.0 / THZ_TO_RAD_S  # rad/s â†’ THz å¤‰æ›ä¿‚æ•°

# --- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ ---
def load_config(config_path: Optional[Union[str, pathlib.Path]] = None) -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(YAML)ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ãƒãƒ¼ã‚¸ã™ã‚‹"""
    if config_path is None:
        config_path = pathlib.Path(__file__).parent / "config_unified.yml"
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šå€¤ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
    default_config = {
        'file_paths': {
            'data_files': [
                {
                    'file': "C:\\Users\\taich\\OneDrive - YNU(ynu.jp)\\master\\ç£æ€§\\GGG\\Programs\\corrected_exp_datasets\\Corrected_Transmittance_Temperature.xlsx",
                    'sheet': "Corrected Data",
                    'type': "auto",
                    'description': "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«"
                }
            ],
            'results_parent_dir': "analysis_results_unified"
        },
        'execution': {},
        'physical_parameters': {
            'B_fixed': 9.0,
            'd_fixed': 157.8e-6,
            's': 3.5,
            'N_spin': 1.9386e+28,
            'initial_values': {
                'eps_bg': 14.20,
                'g_factor': 2.003147,
                'B4': 0.000576,
                'B6': 0.000050,
                'gamma': 0.11e12,
                'a_scale': 0.604971
            }
        },
        'analysis_settings': {
            'low_freq_cutoff': 0.361505,
            'high_freq_cutoff': 0.45,
            'weight_settings': {
                'peak_height_threshold': 0.05,
                'peak_prominence_threshold': 0.05,
                'peak_distance': 10,
                'lp_up_peak_weight': 1.0,
                'between_peaks_weight': 0.1,
                'high_freq_peak_weight': 1.0,
                'background_weight': 0.0
            }
        },
        'mcmc': {
            'draws': 4000,
            'tune': 2000,
            'chains': 4,
            'target_accept': 0.90,
            'init': "adapt_diag",
            'max_iterations': 2
            
        },
        'bayesian_priors': {
            'magnetic_parameters': {
                'a_scale': {'distribution': 'HalfNormal', 'sigma': 1.0},
                'g_factor': {'distribution': 'Normal', 'sigma': 0.1},
                'B4': {'distribution': 'Normal', 'sigma': 0.001},
                'B6': {'distribution': 'Normal', 'sigma': 0.0001}
            },
            'with_prior_info': {
                'a_scale': {'distribution': 'Normal', 'sigma': 0.2},
                'g_factor': {'distribution': 'Normal', 'sigma': 0.05},
                'B4': {'distribution': 'Normal', 'sigma': 0.0005},
                'B6': {'distribution': 'Normal', 'sigma': 0.0001}
            },
            'gamma_parameters': {
                'log_gamma_mu_base': {'distribution': 'Normal', 'sigma': 1.0},
                'log_gamma_sigma_base': {'distribution': 'HalfNormal', 'sigma': 0.8},
                'log_gamma_offset_base': {'distribution': 'Normal', 'mu': 0.0, 'sigma': 0.8, 'shape': 7},
                'temp_gamma_slope': {'distribution': 'Normal', 'mu': 0.0, 'sigma': 0.01}
            },
            'noise_parameters': {
                'sigma': {'distribution': 'HalfNormal', 'sigma': 0.05}
            }
        }
    }
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            user_config = yaml.safe_load(f)
        
        def merge_dict(default, user):
            for key, value in user.items():
                if key in default and isinstance(default[key], dict) and isinstance(value, dict):
                    merge_dict(default[key], value)
                else:
                    default[key] = value
        
        merge_dict(default_config, user_config)
        
        # æ•°å€¤ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹å¤‰æ›ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
        try:
            initial_vals = default_config['physical_parameters']['initial_values']
            for key in ['eps_bg', 'g_factor', 'B4', 'B6', 'gamma', 'a_scale']:
                if key in initial_vals:
                    initial_vals[key] = float(initial_vals[key])
            
            phys_params = default_config['physical_parameters']
            for key in ['B_fixed', 'T_fixed', 'd_fixed', 's', 'N_spin']:
                if key in phys_params:
                    phys_params[key] = float(phys_params[key])
        except (KeyError, ValueError, TypeError) as e:
            print(f"âš ï¸ è­¦å‘Š: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        
        print(f"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        
    except FileNotFoundError:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    return default_config

# --- çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ ---
def create_results_directory(config: Dict[str, Any]) -> pathlib.Path:
    """å®Ÿè¡Œæ—¥æ™‚ã‚’å«ã‚€ä¸€æ„ã®çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    results_parent = config['file_paths']['results_parent_dir']
    results_dir = pathlib.Path(__file__).parent / results_parent / f"run_{timestamp}"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    config_backup_path = results_dir / "config_used.yml"
    with open(config_backup_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ğŸ“ çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {results_dir.resolve()}")
    return results_dir

# --- ç‰©ç†ãƒ¢ãƒ‡ãƒ«é–¢æ•°ç¾¤ (weighted_bayesian_fitting_completed.pyã‹ã‚‰ç§»æ¤) ---

def get_hamiltonian(B_ext_z: float, g_factor: float, B4: float, B6: float) -> np.ndarray:
    """ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’è¨ˆç®—ã™ã‚‹"""
    n_states = int(2 * s + 1)
    m_values = np.arange(s, -s - 1, -1)
    Sz = np.diag(m_values)
    
    if n_states == 8:
        O40 = 60 * np.diag([7, -13, -3, 9, 9, -3, -13, 7])
        X_O44 = np.zeros((8, 8))
        X_O44[3, 7], X_O44[4, 0] = np.sqrt(35), np.sqrt(35)
        X_O44[2, 6], X_O44[5, 1] = 5 * np.sqrt(3), 5 * np.sqrt(3)
        O44 = 12 * (X_O44 + X_O44.T)
        O60 = 1260 * np.diag([1, -5, 9, -5, -5, 9, -5, 1])
        X_O64 = np.zeros((8, 8))
        X_O64[3, 7], X_O64[4, 0] = 3 * np.sqrt(35), 3 * np.sqrt(35)
        X_O64[2, 6], X_O64[5, 1] = -7 * np.sqrt(3), -7 * np.sqrt(3)
        O64 = 60 * (X_O64 + X_O64.T)
    else:
        raise ValueError(f"s={s}ã®çµæ™¶å ´æ¼”ç®—å­ã¯æœªå®Ÿè£…ã§ã™")
    
    H_cf = (B4 * kB) * (O40 + 5 * O44) + (B6 * kB) * (O60 - 21 * O64)
    H_zee = g_factor * muB * B_ext_z * Sz
    return H_cf + H_zee

def normalize_gamma_array(gamma_input, target_length: int = 7) -> np.ndarray:
    """ã‚¬ãƒ³ãƒé…åˆ—ã®æ­£è¦åŒ–ã¨å‹å®‰å…¨æ€§ã‚’ç¢ºä¿"""
    if np.isscalar(gamma_input):
        return np.full(target_length, gamma_input)
    elif hasattr(gamma_input, 'ndim') and gamma_input.ndim == 0:
        return np.full(target_length, float(gamma_input))
    elif hasattr(gamma_input, '__len__'):
        gamma_array = np.array(gamma_input)
        if len(gamma_array) == target_length:
            return gamma_array
        elif len(gamma_array) > target_length:
            return gamma_array[:target_length]
        else:
            return np.pad(gamma_array, (0, target_length - len(gamma_array)), 'edge')
    else:
        return np.full(target_length, gamma_input.item())

def calculate_susceptibility(freq_thz_array: np.ndarray, H: np.ndarray, T: float, 
                             gamma_thz_array: np.ndarray) -> np.ndarray:
    """
    ç£æ°—æ„Ÿå—ç‡ã‚’è¨ˆç®—ã™ã‚‹
    
    Parameters
    ----------
    freq_thz_array : np.ndarray
        å‘¨æ³¢æ•°é…åˆ— [THz]
    H : np.ndarray
        ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¡Œåˆ—
    T : float
        æ¸©åº¦ [K]
    gamma_thz_array : np.ndarray
        ç·©å’Œç‡é…åˆ— [THz]
    
    Returns
    -------
    np.ndarray
        ç£æ°—æ„Ÿå—ç‡ï¼ˆè¤‡ç´ æ•°ï¼‰
    """
    gamma_thz_array = normalize_gamma_array(gamma_thz_array)
    
    # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’å¯¾è§’åŒ–ï¼ˆå›ºæœ‰å€¤ã¯ã‚¨ãƒãƒ«ã‚®ãƒ¼å˜ä½: Jï¼‰
    eigenvalues_J, _ = np.linalg.eigh(H)
    E_min = np.min(eigenvalues_J)
    eigenvalues_shifted_J = eigenvalues_J - E_min  # åŸºåº•çŠ¶æ…‹ã‚’0ã«è¨­å®šï¼ˆJå˜ä½ï¼‰
    
    # ãƒœãƒ«ãƒ„ãƒãƒ³å› å­è¨ˆç®—ï¼ˆç„¡æ¬¡å…ƒåŒ–ï¼‰
    # E / (kB * T) ãŒ700ã‚’è¶…ãˆãªã„ã‚ˆã†ã‚¯ãƒªãƒƒãƒ—
    boltzmann_exponent = np.clip(eigenvalues_shifted_J / (kB * T), -700, 700)
    Z = np.sum(np.exp(-boltzmann_exponent))
    populations = np.exp(-boltzmann_exponent) / Z
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼å·®ï¼ˆJå˜ä½ï¼‰- éš£æ¥æº–ä½é–“ã®é·ç§»
    delta_E_J = eigenvalues_shifted_J[1:] - eigenvalues_shifted_J[:-1]
    delta_pop = populations[1:] - populations[:-1]
    
    valid_mask = np.isfinite(delta_E_J) & (np.abs(delta_E_J) > 1e-30)
    if not np.any(valid_mask):
        return np.zeros_like(freq_thz_array, dtype=complex)
    
    # é·ç§»å‘¨æ³¢æ•°ã‚’THzå˜ä½ã§è¨ˆç®—
    omega_0_rad = delta_E_J / hbar  # rad/s
    freq_0_thz = omega_0_rad * RAD_S_TO_THZ  # THzã«å¤‰æ›
    
    s_val = 3.5
    m_vals = np.arange(s_val, -s_val, -1)
    transition_strength = (s_val + m_vals) * (s_val - m_vals + 1)
    
    if len(gamma_thz_array) != len(delta_E_J):
        if len(gamma_thz_array) > len(delta_E_J):
            gamma_thz_array = gamma_thz_array[:len(delta_E_J)]
        else:
            gamma_thz_array = np.pad(gamma_thz_array, (0, len(delta_E_J) - len(gamma_thz_array)), 'edge')
    
    numerator = delta_pop * transition_strength
    finite_mask = np.isfinite(numerator) & np.isfinite(freq_0_thz) & np.isfinite(gamma_thz_array)
    numerator = numerator[finite_mask]
    freq_0_filtered = freq_0_thz[finite_mask]  # THz
    gamma_filtered = gamma_thz_array[finite_mask]  # THz
    
    if len(numerator) == 0:
        return np.zeros_like(freq_thz_array, dtype=complex)
    
    # ãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨ˆç®—ï¼ˆé«˜é€ŸåŒ–ï¼‰
    # å½¢çŠ¶: freq_thz_array (N_freq,), freq_0_filtered (N_trans,)
    # ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã§ (N_freq, N_trans) ã®2æ¬¡å…ƒé…åˆ—ã¨ã—ã¦è¨ˆç®—
    
    # freq_thz_array[:, None] -> (N_freq, 1)
    # freq_0_filtered[None, :] -> (1, N_trans)
    # çµæœ: (N_freq, N_trans)
    freq_diff = freq_0_filtered[None, :] - freq_thz_array[:, None]  # (N_freq, N_trans)
    denominator = freq_diff - 1j * gamma_filtered[None, :]  # (N_freq, N_trans)
    
    # ã‚¼ãƒ­é™¤ç®—å›é¿
    small_mask = np.abs(denominator) < 1e-10
    denominator[small_mask] = 1e-10 + 1j * 1e-10
    
    # å„å‘¨æ³¢æ•°ã«å¯¾ã—ã¦å…¨é·ç§»ã®å¯„ä¸ã‚’åˆè¨ˆ
    chi_array = -np.sum(numerator[None, :] / denominator, axis=1)  # (N_freq,)
    
    return chi_array

def calculate_normalized_transmission(freq_thz_array: np.ndarray, mu_r_array: np.ndarray, 
                                     d: float, eps_bg: float) -> np.ndarray:
    """
    æ­£è¦åŒ–é€éç‡ã‚’è¨ˆç®—ã™ã‚‹
    
    Parameters
    ----------
    freq_thz_array : np.ndarray
        å‘¨æ³¢æ•°é…åˆ— [THz]
    mu_r_array : np.ndarray
        æ¯”é€ç£ç‡é…åˆ—
    d : float
        è©¦æ–™åšã• [m]
    eps_bg : float
        èƒŒæ™¯èª˜é›»ç‡
    
    Returns
    -------
    np.ndarray
        æ­£è¦åŒ–é€éç‡
    """
    eps_bg = max(eps_bg, 0.1)
    d = max(d, 1e-6)
    
    # THz â†’ rad/s ã«å¤‰æ›ã—ã¦æ³¢é•·ã‚’è¨ˆç®—
    omega_array = freq_thz_array * THZ_TO_RAD_S
    
    mu_r_safe = np.where(np.isfinite(mu_r_array), mu_r_array, 1.0)
    eps_mu_product = eps_bg * mu_r_safe
    eps_mu_product = np.where(eps_mu_product.real > 0, eps_mu_product, 0.1 + 1j * eps_mu_product.imag)
    
    n_complex = np.sqrt(eps_mu_product + 0j)
    impe = np.sqrt(mu_r_safe / eps_bg + 0j)
    
    lambda_0 = np.full_like(omega_array, np.inf, dtype=float)
    nonzero_mask = omega_array > 1e-12
    lambda_0[nonzero_mask] = (2 * np.pi * c) / omega_array[nonzero_mask]
    
    delta = 2 * np.pi * n_complex * d / lambda_0
    delta = np.clip(delta.real, -700, 700) + 1j * np.clip(delta.imag, -700, 700)
    
    numerator = 4 * impe
    exp_pos = np.exp(-1j * delta)
    exp_neg = np.exp(1j * delta)
    
    denominator = (1 + impe)**2 * exp_pos - (1 - impe)**2 * exp_neg
    
    safe_mask = np.abs(denominator) > 1e-15
    t = np.zeros_like(denominator, dtype=complex)
    t[safe_mask] = numerator[safe_mask] / denominator[safe_mask]
    
    transmission = np.abs(t)**2
    transmission = np.where(np.isfinite(transmission), transmission, 0.0)
    transmission = np.clip(transmission, 0, 2)
    
    min_trans, max_trans = np.min(transmission), np.max(transmission)
    
    # æ­£è¦åŒ–: min-maxæ­£è¦åŒ–ã€ãŸã ã—ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå°ã•ã™ãã‚‹å ´åˆã¯è­¦å‘Š
    if max_trans > min_trans and np.isfinite(max_trans) and np.isfinite(min_trans):
        # æ­£è¦åŒ–å®Ÿè¡Œ
        normalized = (transmission - min_trans) / (max_trans - min_trans)
        return normalized
    else:
        # å…¨ã¦åŒã˜å€¤ã®å ´åˆï¼šãƒ‡ãƒãƒƒã‚°æƒ…å ±ãªã—ã§ä¸­é–“å€¤ã‚’è¿”ã™
        # æ³¨æ„: ã“ã®çŠ¶æ…‹ã¯ç‰©ç†ãƒ¢ãƒ‡ãƒ«ã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ã‚’ç¤ºã™
        return np.full_like(transmission, 0.5)

# --- é‡ã¿ä»˜ã‘é–¢æ•° ---
def create_frequency_weights(dataset: Dict[str, Any], analysis_settings: Dict[str, Any]) -> np.ndarray:
    """å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ãƒ”ãƒ¼ã‚¯ç‰¹æ€§ã«åŸºã¥ãã€å°¤åº¦é–¢æ•°ã®ãŸã‚ã®é‡ã¿é…åˆ—ã‚’ç”Ÿæˆã™ã‚‹"""
    weight_config = analysis_settings['weight_settings']
    high_freq_cutoff = analysis_settings['high_freq_cutoff']
    
    freq = dataset['frequency']
    trans = dataset['transmittance_full']
    
    peaks, properties = find_peaks(trans,
                                   height=weight_config['peak_height_threshold'],
                                   prominence=weight_config['peak_prominence_threshold'],
                                   distance=weight_config['peak_distance'])
    
    if len(peaks) < 2:
        weights = np.zeros_like(freq)
        low_freq_mask = freq < high_freq_cutoff
        weights[low_freq_mask] = 1.0
        return weights
    
    widths, _, left_ips, right_ips = peak_widths(trans, peaks, rel_height=0.5)
    left_freq = np.interp(left_ips, np.arange(len(freq)), freq)
    right_freq = np.interp(right_ips, np.arange(len(freq)), freq)
    
    weights = np.full_like(freq, weight_config['background_weight'])
    
    low_freq_peaks = peaks[freq[peaks] < high_freq_cutoff]
    if len(low_freq_peaks) >= 2:
        peak_prominences = properties['prominences'][freq[peaks] < high_freq_cutoff]
        sorted_indices = np.argsort(peak_prominences)[::-1]
        
        lp_idx_in_all_peaks = np.where(peaks == low_freq_peaks[sorted_indices[0]])[0][0]
        up_idx_in_all_peaks = np.where(peaks == low_freq_peaks[sorted_indices[1]])[0][0]
        
        lp_fwhm_right_freq = right_freq[lp_idx_in_all_peaks]
        up_fwhm_left_freq = left_freq[up_idx_in_all_peaks]
        
        lower_bound = np.minimum(lp_fwhm_right_freq, up_fwhm_left_freq)
        upper_bound = np.maximum(lp_fwhm_right_freq, up_fwhm_left_freq)
        between_mask = (freq >= lower_bound) & (freq <= upper_bound)
        weights[between_mask] = weight_config['between_peaks_weight']
        
        lp_fwhm_mask = (freq >= left_freq[lp_idx_in_all_peaks]) & (freq <= right_freq[lp_idx_in_all_peaks])
        up_fwhm_mask = (freq >= left_freq[up_idx_in_all_peaks]) & (freq <= right_freq[up_idx_in_all_peaks])
        weights[lp_fwhm_mask] = weight_config['lp_up_peak_weight']
        weights[up_fwhm_mask] = weight_config['lp_up_peak_weight']
    elif len(low_freq_peaks) == 1:
        # ã€11/25è¿½åŠ ã€‘ ãƒ”ãƒ¼ã‚¯ãŒ1å€‹ã—ã‹ãªã„å ´åˆã®å‡¦ç†
        target_peak = low_freq_peaks[0]
        idx_in_all_peaks = np.where(peaks == target_peak)[0][0]
        
        # ãã®1å€‹ã®ãƒ”ãƒ¼ã‚¯ã®åŠå€¤å¹…é ˜åŸŸã«é‡ã¿ã‚’ä»˜ã‘ã‚‹
        fwhm_mask = (freq >= left_freq[idx_in_all_peaks]) & (freq <= right_freq[idx_in_all_peaks])
        weights[fwhm_mask] = weight_config['lp_up_peak_weight']
        
        print(f"  (Info) ä½å‘¨æ³¢ãƒ”ãƒ¼ã‚¯ãŒ1ã¤ã®ã¿æ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {freq[target_peak]:.3f} THz")
    high_freq_peak_indices = np.where(freq[peaks] >= high_freq_cutoff)[0]
    for idx_in_all_peaks in high_freq_peak_indices:
        fwhm_mask = (freq >= left_freq[idx_in_all_peaks]) & (freq <= right_freq[idx_in_all_peaks])
        weights[fwhm_mask] = weight_config['high_freq_peak_weight']
    
    print(f"  [B={dataset['b_field']:.1f}T, T={dataset['temperature']:.1f}K]: é‡ã¿é…åˆ—ã‚’ç”Ÿæˆ (ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(freq)})")
    return weights

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•° ---
def load_unified_data(config: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:
    """
    è¤‡æ•°ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¸©åº¦å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ç£å ´å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€çš„ã«èª­ã¿è¾¼ã‚€
    
    Parameters
    ----------
    config : Dict[str, Any]
        è¨­å®šè¾æ›¸ï¼ˆfile_paths.data_filesã«è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å«ã‚€ï¼‰
    
    Returns
    -------
    Dict[str, List[Dict[str, Any]]]
        {
            'temp_variable': [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ1, ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ2, ...],  # æ¸©åº¦å¤‰æ•°ã€ç£å ´å›ºå®š
            'field_variable': [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ1, ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ2, ...]  # ç£å ´å¤‰æ•°ã€æ¸©åº¦å›ºå®š
        }
    """
    print("\n--- çµ±åˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ) ---")
    
    all_temp_datasets = []
    all_field_datasets = []
    
    # æ—§å½¢å¼(å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«)ã¨ã®å¾Œæ–¹äº’æ›æ€§
    if 'data_file' in config['file_paths']:
        print("âš ï¸ æ—§å½¢å¼ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™ã€‚")
        file_configs = [{
            'file': config['file_paths']['data_file'],
            'sheet': config['file_paths'].get('sheet_name', 'Corrected Data'),
            'description': 'å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«(äº’æ›ãƒ¢ãƒ¼ãƒ‰)'
        }]
    else:
        file_configs = config['file_paths'].get('data_files', [])
    
    if not file_configs:
        raise ValueError("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«data_filesãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    freq_col = 'Frequency (THz)'
    B_fixed = config['physical_parameters']['B_fixed']
    T_fixed = config['physical_parameters'].get('T_fixed', 4.0)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    for file_idx, file_config in enumerate(file_configs, 1):
        file_path = file_config['file']
        sheet_name = file_config['sheet']
        description = file_config.get('description', '')
        
        print(f"\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ« {file_idx}/{len(file_configs)}: {pathlib.Path(file_path).name}")
        if description:
            print(f"   èª¬æ˜: {description}")
        print(f"   ã‚·ãƒ¼ãƒˆ: {sheet_name}")
        
        try:
            df = pd.read_excel(file_path, sheet_name=sheet_name, header=0)
        except Exception as e:
            print(f"   âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"   ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
        
        df[freq_col] = pd.to_numeric(df[freq_col], errors='coerce')
        
        # æ¸©åº¦å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆ'K'ã§çµ‚ã‚ã‚‹åˆ—ã‚’è‡ªå‹•æ¤œå‡ºï¼‰
        temp_cols = [col for col in df.columns if col.endswith('K') and col != freq_col]
        
        if temp_cols:
            print(f"   ğŸ“Š æ¸©åº¦å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ (B={B_fixed}Tå›ºå®š)")
            print(f"      æ¤œå‡ºã•ã‚ŒãŸæ¸©åº¦åˆ—: {temp_cols}")
            
            for col in temp_cols:
                try:
                    temp_value = float(col.replace('K', ''))
                    df_clean = df[[freq_col, col]].dropna()
                    freq, trans = df_clean[freq_col].values.astype(np.float64), df_clean[col].values.astype(np.float64)
                    
                    all_temp_datasets.append({
                        'temperature': temp_value,
                        'b_field': B_fixed,
                        'frequency': freq,  # THzå˜ä½
                        'transmittance_full': trans,
                        'pattern': 'temp_variable',
                        'source_file': pathlib.Path(file_path).name
                    })
                    print(f"      âœ“ T={temp_value}K, B={B_fixed}T (ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(freq)})")
                except ValueError:
                    print(f"      âš ï¸ åˆ— '{col}' ã¯æ¸©åº¦ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è§£é‡ˆã§ãã¾ã›ã‚“ã€‚")
        
        # ç£å ´å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ï¼ˆ'T'ã§çµ‚ã‚ã‚‹åˆ—ã‚’è‡ªå‹•æ¤œå‡ºã€æ¸©åº¦åˆ—ã‚’é™¤å¤–ï¼‰
        field_cols = [col for col in df.columns if col.endswith('T') and col != freq_col and col not in temp_cols]
        
        if field_cols:
            print(f"   ğŸ“Š ç£å ´å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿ (T={T_fixed}Kå›ºå®š)")
            print(f"      æ¤œå‡ºã•ã‚ŒãŸç£å ´åˆ—: {field_cols}")
            
            for col in field_cols:
                try:
                    B_value = float(col.replace('T', ''))
                    df_clean = df[[freq_col, col]].dropna()
                    freq, trans = df_clean[freq_col].values.astype(np.float64), df_clean[col].values.astype(np.float64)
                    
                    all_field_datasets.append({
                        'temperature': T_fixed,
                        'b_field': B_value,
                        'frequency': freq,  # THzå˜ä½
                        'transmittance_full': trans,
                        'pattern': 'field_variable',
                        'source_file': pathlib.Path(file_path).name
                    })
                    print(f"      âœ“ T={T_fixed}K, B={B_value}T (ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(freq)})")
                except ValueError:
                    print(f"      âš ï¸ åˆ— '{col}' ã¯ç£å ´ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è§£é‡ˆã§ãã¾ã›ã‚“ã€‚")
    
    print(f"\n" + "="*70)
    print(f"âœ… å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿å®Œäº†:")
    print(f"  - æ¸©åº¦å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿: {len(all_temp_datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    print(f"  - ç£å ´å¤‰åŒ–ãƒ‡ãƒ¼ã‚¿: {len(all_field_datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    print(f"="*70)
    
    return {
        'temp_variable': all_temp_datasets,
        'field_variable': all_field_datasets
    }

def split_data_by_frequency(datasets: List[Dict[str, Any]], 
                           low_cutoff: float, high_cutoff: float) -> Dict[str, List[Dict[str, Any]]]:
    """ãƒ‡ãƒ¼ã‚¿ã‚’å‘¨æ³¢æ•°å¸¯åŸŸã§åˆ†å‰²"""
    low_freq_datasets = []
    high_freq_datasets = []
    
    for data in datasets:
        freq = data['frequency']
        trans = data['transmittance_full']
        
        # é«˜å‘¨æ³¢é ˜åŸŸ
        high_mask = freq >= high_cutoff
        if np.any(high_mask):
            min_high, max_high = trans[high_mask].min(), trans[high_mask].max()
            trans_norm_high = (trans[high_mask] - min_high) / (max_high - min_high) if max_high > min_high else np.full_like(trans[high_mask], 0.5)
            high_freq_datasets.append({
                'temperature': data['temperature'],
                'b_field': data['b_field'],
                'frequency': freq[high_mask],  # THzå˜ä½
                'transmittance': trans_norm_high,
                'pattern': data['pattern']
            })
    
    return {
        'high_freq': high_freq_datasets,
        'all_full': datasets
    }

# --- eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° ---
def get_eps_bg_initial_values_and_bounds(temperature: float) -> Tuple[List[float], Tuple[float, float]]:
    """æ¸©åº¦ä¾å­˜eps_bgåˆæœŸå€¤ã¨å¢ƒç•Œå€¤ã®å–å¾—"""
    eps_bg_init = 14.20
    if temperature <= 10:
        initial_eps_bg_values = [eps_bg_init * 0.85, eps_bg_init * 0.90, eps_bg_init * 0.95, eps_bg_init,
                                13.0, 12.5, 12.8, 13.2, 13.5, 14.0]
        bounds_eps_bg = (11.0, 16.0)
    elif temperature <= 100:
        initial_eps_bg_values = [eps_bg_init * 0.98, eps_bg_init, eps_bg_init * 1.02, eps_bg_init * 1.05,
                                13.8, 14.0, 14.2, 13.5, 14.5, 13.2]
        bounds_eps_bg = (11.5, 16.5)
    else:
        initial_eps_bg_values = [eps_bg_init * 1.05, eps_bg_init * 1.10, eps_bg_init * 1.15, eps_bg_init,
                                14.5, 15.0, 15.5, 14.0, 16.0, 13.8]
        bounds_eps_bg = (12.0, 17.0)
    return initial_eps_bg_values, bounds_eps_bg

def fit_eps_bg_unified(dataset: Dict[str, Any], 
                      bayesian_params: Optional[Dict[str, float]] = None,
                      config: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
    """çµ±åˆeps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆç£å ´ãƒ»æ¸©åº¦ä¸¡å¯¾å¿œï¼‰"""
    if config is None:
        raise ValueError("config ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å¿…é ˆã§ã™")
    
    T = dataset['temperature']
    B = dataset['b_field']
    d_fixed = config['physical_parameters']['d_fixed']
    N_spin = config['physical_parameters']['N_spin']
    
    print(f"\n--- eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° [B={B:.1f}T, T={T:.1f}K] ---")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
    g_factor: float
    B4: float
    B6: float
    a_scale: float
    
    if bayesian_params is not None:
        g_factor = bayesian_params.get('g_factor') or config['physical_parameters']['initial_values']['g_factor']
        B4 = bayesian_params.get('B4') or config['physical_parameters']['initial_values']['B4']
        B6 = bayesian_params.get('B6') or config['physical_parameters']['initial_values']['B6']
        a_scale = bayesian_params.get('a_scale') or config['physical_parameters']['initial_values']['a_scale']
        print(f"  ğŸ”„ ãƒ™ã‚¤ã‚ºæ¨å®šçµæœã‚’ä½¿ç”¨")
    else:
        g_factor = config['physical_parameters']['initial_values']['g_factor']
        B4 = config['physical_parameters']['initial_values']['B4']
        B6 = config['physical_parameters']['initial_values']['B6']
        a_scale = config['physical_parameters']['initial_values']['a_scale']
        print(f"  ğŸ”° åˆæœŸå€¤ã‚’ä½¿ç”¨")
    
    def model_func(freq_thz, eps_bg_fit):
        """eps_bgã®ã¿ã‚’å¤‰æ•°ã¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆTHzå˜ä½ç³»ï¼‰"""
        try:
            H = get_hamiltonian(B, g_factor, B4, B6)
            # gammaã‚‚THzå˜ä½ã§æŒ‡å®šï¼ˆ0.11 THz â‰ˆ 0.11e12 rad/s Ã· 2Ï€Ã—10^12ï¼‰
            gamma_thz_array = np.full(7, 0.018)  # ç´„0.018 THz = 0.11e12 rad/s
            chi_raw = calculate_susceptibility(freq_thz, H, T, gamma_thz_array)
            
            G0 = a_scale * mu0 * N_spin * (g_factor * muB)**2 / (2 * hbar)
            chi = G0 * chi_raw
            mu_r = 1 + chi  # H_formï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´ï¼‰
            
            return calculate_normalized_transmission(freq_thz, mu_r, d_fixed, eps_bg_fit)
        except:
            return np.ones_like(freq_thz) * 0.5
    
    # åˆæœŸå€¤ã¨å¢ƒç•Œ
    initial_eps_bg_values, bounds_eps_bg = get_eps_bg_initial_values_and_bounds(T)
    
    for attempt, initial_eps_bg in enumerate(initial_eps_bg_values):
        try:
            popt, _ = curve_fit(model_func, dataset['frequency'], dataset['transmittance'],
                               p0=[initial_eps_bg], bounds=([bounds_eps_bg[0]], [bounds_eps_bg[1]]),
                               maxfev=3000, method='trf')
            eps_bg_fit = popt[0]
            
            if bounds_eps_bg[0] <= eps_bg_fit <= bounds_eps_bg[1]:
                print(f"  âœ… æˆåŠŸ: eps_bg = {eps_bg_fit:.3f}")
                return {
                    'eps_bg': eps_bg_fit,
                    'd': d_fixed,
                    'temperature': T,
                    'b_field': B
                }
        except:
            continue
    
    print(f"  âŒ å…¨è©¦è¡Œå¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨")
    return {'eps_bg': 14.20, 'd': d_fixed, 'temperature': T, 'b_field': B}

# --- PyMC Op ã‚¯ãƒ©ã‚¹ï¼ˆçµ±åˆç‰ˆãƒ»THzå˜ä½ç³»ï¼‰ ---
class UnifiedMagneticModelOp(Op):
    """
    ç£å ´ãƒ»æ¸©åº¦ä¸¡å¯¾å¿œã®çµ±åˆPyMC Opã‚¯ãƒ©ã‚¹
    
    å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å˜ä½:
    - a_scale: ç„¡æ¬¡å…ƒ
    - gamma_concat: THzå˜ä½
    - g_factor: ç„¡æ¬¡å…ƒ
    - B4, B6: Kå˜ä½
    """
    def __init__(self, datasets: List[Dict[str, Any]], 
                 bt_specific_params: Dict[Tuple[float, float], Dict[str, float]], 
                 model_type: str):
        super().__init__()
        self.datasets = datasets
        self.bt_specific_params = bt_specific_params  # (B, T) -> {'eps_bg', 'd'}
        self.model_type = model_type
        
        # å…¨ã¦ã®(B, T)ãƒšã‚¢ã‚’å–å¾—ã—ã¦ã‚½ãƒ¼ãƒˆ
        self.bt_pairs = sorted(list(set([(d['b_field'], d['temperature']) for d in datasets])))
        
        self.itypes = [pt.dscalar, pt.dvector, pt.dscalar, pt.dscalar, pt.dscalar]
        self.otypes = [pt.dvector]
    
    def perform(self, node, inputs, output_storage):
        a_scale, gamma_thz_concat, g_factor, B4, B6 = inputs
        full_predicted_y = []
        gamma_start_idx = 0
        
        for data in self.datasets:
            B = data['b_field']
            T = data['temperature']
            freq_thz = data['frequency']  # THzå˜ä½
            
            # (B, T)ã«å¯¾å¿œã™ã‚‹eps_bgã¨dã‚’å–å¾—
            bt_key = (B, T)
            if bt_key in self.bt_specific_params:
                d_fixed = self.bt_specific_params[bt_key]['d']
                eps_bg_fixed = self.bt_specific_params[bt_key]['eps_bg']
            else:
                d_fixed = 157.8e-6
                eps_bg_fixed = 14.20
            
            # æ¸©åº¦ä¾å­˜gammaã®å–å¾—ï¼ˆTHzå˜ä½ï¼‰
            gamma_end_idx = gamma_start_idx + 7
            gamma_thz_for_bt = gamma_thz_concat[gamma_start_idx:gamma_end_idx]
            gamma_start_idx = gamma_end_idx
            
            # ç‰©ç†ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ï¼ˆå…¨ã¦THzå˜ä½ï¼‰
            H = get_hamiltonian(B, g_factor, B4, B6)
            chi_raw = calculate_susceptibility(freq_thz, H, T, gamma_thz_for_bt)
            
            G0 = a_scale * mu0 * 1.9386e+28 * (g_factor * muB)**2 / (2 * hbar)
            chi = G0 * chi_raw
            
            if self.model_type == 'B_form':
                mu_r = 1 / (1 - chi)
            else:  # H_form
                mu_r = 1 + chi
            
            predicted_trans = calculate_normalized_transmission(freq_thz, mu_r, d_fixed, eps_bg_fixed)
            predicted_trans = np.where(np.isfinite(predicted_trans), predicted_trans, 0.5)
            predicted_trans = np.clip(predicted_trans, 0, 1)
            
            full_predicted_y.extend(predicted_trans)
        
        output_storage[0][0] = np.array(full_predicted_y)

# --- ãƒ™ã‚¤ã‚ºæ¨å®šé–¢æ•° ---
def create_single_prior(name: str, config: Dict[str, Any], mu: Optional[float] = None) -> Any:
    """
    è¨­å®šã«åŸºã¥ã„ã¦å˜ä¸€ã®äº‹å‰åˆ†å¸ƒã‚’ä½œæˆã™ã‚‹æ±ç”¨é–¢æ•°
    
    Parameters
    ----------
    name : str
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å
    config : Dict[str, Any]
        åˆ†å¸ƒè¨­å®šï¼ˆdistribution, mu, sigma, lower, upperç­‰ï¼‰
    mu : Optional[float]
        ä¸­å¿ƒå€¤ï¼ˆconfigã«muãŒãªã„å ´åˆã«ä½¿ç”¨ï¼‰
    
    Returns
    -------
    PyMCåˆ†å¸ƒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    dist_type = config.get('distribution', 'Normal')
    sigma = config['sigma']
    
    # muã®æ±ºå®š: config > å¼•æ•° > ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.0
    mu_value = config.get('mu', mu if mu is not None else 0.0)
    
    if dist_type == 'Normal':
        return pm.Normal(name, mu=mu_value, sigma=sigma)
    
    elif dist_type == 'HalfNormal':
        return pm.HalfNormal(name, sigma=sigma)
    
    elif dist_type == 'TruncatedNormal':
        # æ­£å€¤åˆ¶ç´„ä»˜ãæ­£è¦åˆ†å¸ƒï¼ˆç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ¨å¥¨ï¼‰
        lower = config.get('lower', 0.0)
        upper = config.get('upper', None)
        return pm.TruncatedNormal(name, mu=mu_value, sigma=sigma, lower=lower, upper=upper)
    
    elif dist_type == 'LogNormal':
        # å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼ˆæ­£å€¤ã®ã¿ã€ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é©ã™ã‚‹ï¼‰
        # mu, sigmaã¯å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å€¤ã¨ã—ã¦è§£é‡ˆ
        return pm.LogNormal(name, mu=np.log(mu_value) if mu_value > 0 else 0.0, sigma=sigma)
    
    else:
        raise ValueError(f"æœªå¯¾å¿œã®åˆ†å¸ƒã‚¿ã‚¤ãƒ—: {dist_type}")


def create_prior_distributions(prior_config: Dict[str, Any], 
                              prior_magnetic_params: Optional[Dict[str, float]] = None,
                              initial_values: Optional[Dict[str, float]] = None) -> Dict[str, Any]:
    """
    äº‹å‰åˆ†å¸ƒã‚’ä½œæˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®åˆ†å¸ƒã‚¿ã‚¤ãƒ—èª­ã¿å–ã‚Šã«å¯¾å¿œï¼‰
    
    å¯¾å¿œåˆ†å¸ƒ:
    - Normal: é€šå¸¸ã®æ­£è¦åˆ†å¸ƒ
    - HalfNormal: åŠæ­£è¦åˆ†å¸ƒï¼ˆæ­£å€¤ã®ã¿ã€Î¼=0ï¼‰
    - TruncatedNormal: åˆ‡æ–­æ­£è¦åˆ†å¸ƒï¼ˆæŒ‡å®šç¯„å›²ã«åˆ¶ç´„ï¼‰
    - LogNormal: å¯¾æ•°æ­£è¦åˆ†å¸ƒï¼ˆæ­£å€¤ã®ã¿ï¼‰
    """
    priors = {}
    
    if prior_magnetic_params is None:
        # åˆå›å®Ÿè¡Œæ™‚: initial_valuesã‚’ä¸­å¿ƒå€¤ã¨ã—ã¦ä½¿ç”¨
        if initial_values is None:
            raise ValueError("initial_values ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ prior_magnetic_params ãŒ None ã®å ´åˆã«å¿…é ˆã§ã™")
        mag_config = prior_config['magnetic_parameters']
        
        # a_scale: æ­£å€¤ã®ã¿ï¼ˆTruncatedNormalã¾ãŸã¯HalfNormalæ¨å¥¨ï¼‰
        priors['a_scale'] = create_single_prior('a_scale', mag_config['a_scale'], 
                                                mu=initial_values.get('a_scale', 1.0))
        
        # g_factor: æ­£å€¤ã®ã¿ï¼ˆTruncatedNormalæ¨å¥¨ï¼‰
        priors['g_factor'] = create_single_prior('g_factor', mag_config['g_factor'],
                                                 mu=initial_values['g_factor'])
        
        # B4, B6: æ­£è² ä¸¡æ–¹ã‚ã‚Šå¾—ã‚‹ï¼ˆNormalï¼‰
        priors['B4'] = create_single_prior('B4', mag_config['B4'], mu=initial_values['B4'])
        priors['B6'] = create_single_prior('B6', mag_config['B6'], mu=initial_values['B6'])
    else:
        # 2å›ç›®ä»¥é™: å‰å›ã®æ¨å®šçµæœã‚’ä¸­å¿ƒå€¤ã¨ã—ã¦ä½¿ç”¨
        prior_config_info = prior_config['with_prior_info']
        
        priors['a_scale'] = create_single_prior('a_scale', prior_config_info['a_scale'],
                                                mu=prior_magnetic_params['a_scale'])
        priors['g_factor'] = create_single_prior('g_factor', prior_config_info['g_factor'],
                                                 mu=prior_magnetic_params['g_factor'])
        priors['B4'] = create_single_prior('B4', prior_config_info['B4'],
                                          mu=prior_magnetic_params['B4'])
        priors['B6'] = create_single_prior('B6', prior_config_info['B6'],
                                          mu=prior_magnetic_params['B6'])
    
    return priors

def create_gamma_priors(gamma_config: Dict[str, Any], gamma_thz_init: float) -> Dict[str, Any]:
    """
    gammaäº‹å‰åˆ†å¸ƒã‚’ä½œæˆï¼ˆTHzå˜ä½ï¼‰
    
    Parameters
    ----------
    gamma_config : Dict[str, Any]
        gammaé–¢é€£ã®äº‹å‰åˆ†å¸ƒè¨­å®š
    gamma_thz_init : float
        gammaåˆæœŸå€¤ [THz]
    
    Returns
    -------
    Dict[str, Any]
        gammaäº‹å‰åˆ†å¸ƒã®PyMCã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    gamma_thz_init_float = float(gamma_thz_init)
    if gamma_thz_init_float <= 0:
        raise ValueError(f"gamma_thz_init must be positive, got {gamma_thz_init_float}")
    
    gamma_priors = {}
    # log(gamma)ã®ãƒ™ãƒ¼ã‚¹å€¤ï¼ˆTHzå˜ä½ã§ã®å¯¾æ•°ï¼‰
    gamma_priors['log_gamma_mu_base'] = pm.Normal('log_gamma_mu_base', 
                                                  mu=np.log(gamma_thz_init_float), 
                                                  sigma=gamma_config['log_gamma_mu_base']['sigma'])
    gamma_priors['log_gamma_sigma_base'] = pm.HalfNormal('log_gamma_sigma_base', 
                                                         sigma=gamma_config['log_gamma_sigma_base']['sigma'])
    gamma_priors['log_gamma_offset_base'] = pm.Normal('log_gamma_offset_base', 
                                                      mu=gamma_config['log_gamma_offset_base']['mu'], 
                                                      sigma=gamma_config['log_gamma_offset_base']['sigma'], 
                                                      shape=gamma_config['log_gamma_offset_base']['shape'])
    gamma_priors['temp_gamma_slope'] = pm.Normal('temp_gamma_slope', 
                                                 mu=gamma_config['temp_gamma_slope']['mu'], 
                                                 sigma=gamma_config['temp_gamma_slope']['sigma'])
    return gamma_priors

def run_unified_bayesian_fit(datasets: List[Dict[str, Any]], 
                             bt_specific_params: Dict[Tuple[float, float], Dict[str, float]],
                             weights_list: List[np.ndarray], 
                             results_dir: pathlib.Path,
                             config: Dict[str, Any],
                             prior_magnetic_params: Optional[Dict[str, float]] = None, 
                             model_type: str = 'H_form') -> Optional[az.InferenceData]:
    """çµ±åˆãƒ™ã‚¤ã‚ºæ¨å®šï¼ˆç£å ´ãƒ»æ¸©åº¦ä¸€æ‹¬å‡¦ç†ï¼‰"""
    print(f"\n{'='*70}")
    print(f"çµ±åˆãƒ™ã‚¤ã‚ºæ¨å®šå®Ÿè¡Œ (ãƒ¢ãƒ‡ãƒ«: {model_type})")
    print(f"{'='*70}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ•°: {len(datasets)}")
    print(f"(B, T)ãƒšã‚¢æ•°: {len(set([(d['b_field'], d['temperature']) for d in datasets]))}")
    
    combined_weights = np.concatenate(weights_list)
    
    with pm.Model() as model:
        prior_config = config['bayesian_priors']
        initial_values = config['physical_parameters']['initial_values']
        
        # ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äº‹å‰åˆ†å¸ƒ
        magnetic_priors = create_prior_distributions(prior_config, prior_magnetic_params, initial_values)
        a_scale = magnetic_priors['a_scale']
        g_factor = magnetic_priors['g_factor']
        B4 = magnetic_priors['B4']
        B6 = magnetic_priors['B6']
        
        # gammaäº‹å‰åˆ†å¸ƒï¼ˆTHzå˜ä½ï¼‰
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®gammaåˆæœŸå€¤ã‚’THzå˜ä½ã«å¤‰æ›
        gamma_init_raw = initial_values['gamma']
        # gamma_init_rawãŒrad/så˜ä½ã®å ´åˆï¼ˆ>1e9ï¼‰ã€THzå˜ä½ã«å¤‰æ›
        if gamma_init_raw > 1e9:
            gamma_thz_init = gamma_init_raw * RAD_S_TO_THZ
            print(f"  gammaåˆæœŸå€¤: {gamma_init_raw:.2e} rad/s â†’ {gamma_thz_init:.4f} THz ã«å¤‰æ›")
        else:
            gamma_thz_init = gamma_init_raw
            print(f"  gammaåˆæœŸå€¤: {gamma_thz_init:.4f} THz (æ—¢ã«THzå˜ä½)")
        
        gamma_priors = create_gamma_priors(prior_config['gamma_parameters'], 
                                          gamma_thz_init)
        log_gamma_mu_base = gamma_priors['log_gamma_mu_base']
        log_gamma_sigma_base = gamma_priors['log_gamma_sigma_base']
        log_gamma_offset_base = gamma_priors['log_gamma_offset_base']
        temp_gamma_slope = gamma_priors['temp_gamma_slope']
        
        # å…¨(B, T)ãƒšã‚¢ã§ã®gammaè¨ˆç®—ï¼ˆTHzå˜ä½ï¼‰
        bt_pairs = sorted(list(set([(d['b_field'], d['temperature']) for d in datasets])))
        gamma_thz_all_bt = []
        base_temp = 4.0
        
        for B, T in bt_pairs:
            temp_diff = T - base_temp
            temp_correction = temp_gamma_slope * temp_diff
            log_gamma_mu_temp = log_gamma_mu_base + temp_correction
            # gamma_bt ã¯THzå˜ä½
            gamma_thz_bt = pt.exp(log_gamma_mu_temp + log_gamma_offset_base * log_gamma_sigma_base)
            gamma_thz_all_bt.append(gamma_thz_bt)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é †åºã«åˆã‚ã›ã¦gammaã‚’é¸æŠ
        gamma_thz_final = []
        for dataset in datasets:
            bt_key = (dataset['b_field'], dataset['temperature'])
            bt_idx = bt_pairs.index(bt_key)
            gamma_thz_final.append(gamma_thz_all_bt[bt_idx])
        
        gamma_thz_concat = pt.concatenate(gamma_thz_final, axis=0)
        
        # é‡ã¿ä»˜ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆfrequencyä½¿ç”¨ã€omegaã¯ä¸è¦ï¼‰
        datasets_weighted = []
        weights_start_idx = 0
        for i, data in enumerate(datasets):
            n_points = len(data['transmittance_full'])
            weights_end_idx = weights_start_idx + n_points
            
            dataset_weights = combined_weights[weights_start_idx:weights_end_idx]
            dataset_valid_indices = np.where(dataset_weights > 0)[0]
            
            if len(dataset_valid_indices) > 0:
                weighted_dataset = {
                    'temperature': data['temperature'],
                    'b_field': data['b_field'],
                    'frequency': data['frequency'][dataset_valid_indices],  # THzå˜ä½
                    'transmittance_full': data['transmittance_full'][dataset_valid_indices],
                    'weights': dataset_weights[dataset_valid_indices],
                    'pattern': data['pattern']
                }
                datasets_weighted.append(weighted_dataset)
            
            weights_start_idx = weights_end_idx
        
        if not datasets_weighted:
            print("âš ï¸ æœ‰åŠ¹ãªé‡ã¿ä»˜ããƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return None
        
        # PyMC Opï¼ˆgamma_thz_concatã¯THzå˜ä½ï¼‰
        op_weighted = UnifiedMagneticModelOp(datasets_weighted, bt_specific_params, model_type)
        mu = op_weighted(a_scale, gamma_thz_concat, g_factor, B4, B6)
        
        weights_tensor = pt.as_tensor_variable(np.concatenate([d['weights'] for d in datasets_weighted]))
        
        noise_config = prior_config['noise_parameters']['sigma']
        sigma = pm.HalfNormal('sigma', sigma=noise_config['sigma'])
        sigma_adjusted = sigma / pt.sqrt(weights_tensor)
        
        trans_target = np.concatenate([d['transmittance_full'] for d in datasets_weighted])
        Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma_adjusted, observed=trans_target)
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        mcmc_config = config['mcmc']
        try:
            # ä¸¦åˆ—ã‚³ã‚¢æ•°ã®æ±ºå®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ« > ãƒã‚§ãƒ¼ãƒ³æ•° > è‡ªå‹•æ¤œå‡ºï¼‰
            n_cores = mcmc_config.get('cores', mcmc_config['chains'])
            if n_cores == 'auto':
                import multiprocessing
                n_cores = min(multiprocessing.cpu_count(), mcmc_config['chains'])
            
            sample_kwargs = {
                'draws': mcmc_config['draws'],
                'tune': mcmc_config['tune'],
                'chains': mcmc_config['chains'],
                'cores': n_cores,  # ä¸¦åˆ—å®Ÿè¡Œã‚³ã‚¢æ•°
                'target_accept': mcmc_config['target_accept'],
                'random_seed': mcmc_config.get('random_seed', None),
                'init': mcmc_config.get('init', 'auto'),
                'return_inferencedata': True,
                'progressbar': True,
                'idata_kwargs': {'log_likelihood': True}
            }
            
            print(f"âš¡ ä¸¦åˆ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: {mcmc_config['chains']}ãƒã‚§ãƒ¼ãƒ³ Ã— {n_cores}ã‚³ã‚¢")

            if 'nuts_sampler' in mcmc_config:
                sample_kwargs['nuts_sampler'] = mcmc_config['nuts_sampler']
                print(f"ğŸš€ é«˜é€Ÿã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’ä½¿ç”¨: {mcmc_config['nuts_sampler']}")
            
            trace = pm.sample(**sample_kwargs)
            print("âœ… ãƒ™ã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
            
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    
    # çµæœä¿å­˜
    trace_filename = results_dir / f'trace_{model_type}.nc'
    az.to_netcdf(trace, trace_filename)
    print(f"âœ… Traceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜: {trace_filename}")
    
    return trace

def extract_bayesian_parameters(trace: az.InferenceData) -> Dict[str, float]:
    """ãƒ™ã‚¤ã‚ºæ¨å®šçµæœã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º"""
    posterior = trace["posterior"]
    a_scale_mean = posterior['a_scale'].mean().item()
    g_factor_mean = posterior['g_factor'].mean().item()
    result = {
        'a_scale': a_scale_mean,
        'g_factor': g_factor_mean,
        'B4': posterior['B4'].mean().item(),
        'B6': posterior['B6'].mean().item(),
        'G0': a_scale_mean * mu0 * 1.9386e+28 * (g_factor_mean * muB)**2 / (2 * hbar)
    }
    
    try:
        result['log_gamma_mu_base'] = posterior['log_gamma_mu_base'].mean().item()
        result['temp_gamma_slope'] = posterior['temp_gamma_slope'].mean().item()
    except KeyError:
        pass
    
    return result

# --- çµæœä¿å­˜é–¢æ•° ---
def save_unified_results(final_traces: Dict[str, az.InferenceData], 
                        bt_params: Dict[str, Dict[Tuple[float, float], Dict[str, float]]],
                        results_dir: pathlib.Path):
    """çµ±åˆçµæœã®ä¿å­˜"""
    print("\n--- çµæœã‚’CSVã«ä¿å­˜ä¸­ ---")
    
    for model_type, trace in final_traces.items():
        params = extract_bayesian_parameters(trace)
        params_df = pd.DataFrame([params])
        params_file = results_dir / f'fitting_parameters_{model_type}.csv'
        params_df.to_csv(params_file, index=False)
        print(f"âœ… ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {params_file}")
    
    for model_type, bt_specific_params in bt_params.items():
        bt_params_list = []
        for (B, T), params in sorted(bt_specific_params.items()):
            bt_params_list.append(params)
        
        if bt_params_list:
            bt_df = pd.DataFrame(bt_params_list)
            bt_file = results_dir / f'bt_optical_parameters_{model_type}.csv'
            bt_df.to_csv(bt_file, index=False)
            print(f"âœ… {model_type}ã®(B,T)åˆ¥å…‰å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {bt_file}")

# --- ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ---
def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("="*70)
    print("ç£å ´ãƒ»æ¸©åº¦ä¸€æ‹¬ãƒ™ã‚¤ã‚ºæ¨å®šãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
    print("="*70)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    
    # ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š
    if 'random_seed' in config['mcmc']:
        RANDOM_SEED = config['mcmc']['random_seed']
        np.random.seed(RANDOM_SEED)
        print(f"ğŸ² ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®š: {RANDOM_SEED}")
    
    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    results_dir = create_results_directory(config)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n" + "="*70)
    print("ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    print("="*70)
    
    unified_data = load_unified_data(config)
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çµåˆ
    all_datasets = unified_data['temp_variable'] + unified_data['field_variable']
    
    if not all_datasets:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚çµ‚äº†ã—ã¾ã™ã€‚")
        return
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’å‘¨æ³¢æ•°å¸¯åŸŸã§åˆ†å‰²
    split_data = split_data_by_frequency(
        all_datasets,
        config['analysis_settings']['low_freq_cutoff'],
        config['analysis_settings']['high_freq_cutoff']
    )
    
    high_freq_datasets = split_data['high_freq']
    all_datasets_full = split_data['all_full']
    
    # åˆå›eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    print("\n" + "="*70)
    print("ã‚¹ãƒ†ãƒƒãƒ—2: åˆå›eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°")
    print("="*70)
    
    bt_specific_params = {}
    for dataset in high_freq_datasets:
        result = fit_eps_bg_unified(dataset, bayesian_params=None, config=config)
        bt_key = (result['b_field'], result['temperature'])
        bt_specific_params[bt_key] = result
    
    # é‡ã¿é…åˆ—ç”Ÿæˆ
    print("\n" + "="*70)
    print("ã‚¹ãƒ†ãƒƒãƒ—3: é‡ã¿é…åˆ—ç”Ÿæˆ")
    print("="*70)
    
    weights_list = [create_frequency_weights(d, config['analysis_settings']) for d in all_datasets_full]
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ãƒ«ãƒ¼ãƒ—
    final_traces = {}
    model_bt_params = {
        'H_form': bt_specific_params.copy(),
        'B_form': bt_specific_params.copy()
    }
    max_iterations = config['mcmc']['max_iterations']
    
    for iteration in range(max_iterations):
        print(f"\n{'='*70}")
        print(f"åå¾© {iteration + 1}/{max_iterations}")
        print(f"{'='*70}")
        
        for model_type in ['H_form', 'B_form']:
            print(f"\n{model_type}ãƒ¢ãƒ‡ãƒ«ã®å‡¦ç†")
            
            model_specific_prior = None
            if iteration > 0 and model_type in final_traces:
                model_specific_prior = extract_bayesian_parameters(final_traces[model_type])
                print(f"  ğŸ“Œ å‰å›ã®{model_type}çµæœã‚’äº‹å‰åˆ†å¸ƒã¨ã—ã¦ä½¿ç”¨")
            
            trace = run_unified_bayesian_fit(
                all_datasets_full,
                model_bt_params[model_type],
                weights_list,
                results_dir,
                config,
                prior_magnetic_params=model_specific_prior,
                model_type=model_type
            )
            
            if trace:
                final_traces[model_type] = trace
                
                # eps_bgæ›´æ–°ï¼ˆæœ€å¾Œã®åå¾©ã§ãªã„å ´åˆï¼‰
                if iteration < max_iterations - 1:
                    print(f"\n{model_type}ã®eps_bgæ›´æ–°ä¸­...")
                    bayesian_params = extract_bayesian_parameters(trace)
                    updated_bt_params = {}
                    
                    for dataset in high_freq_datasets:
                        result = fit_eps_bg_unified(dataset, bayesian_params=bayesian_params, config=config)
                        bt_key = (result['b_field'], result['temperature'])
                        updated_bt_params[bt_key] = result
                    
                    model_bt_params[model_type] = updated_bt_params
    
    # çµæœä¿å­˜
    if final_traces:
        print(f"\n{'='*70}")
        print("æœ€çµ‚çµæœã®ä¿å­˜")
        print(f"{'='*70}")
        save_unified_results(final_traces, model_bt_params, results_dir)
        
        print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        print(f"ğŸ“ çµæœã¯ '{results_dir}' ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™")
    else:
        print("âŒ ãƒ™ã‚¤ã‚ºæ¨å®šãŒå¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    import multiprocessing
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass
    
    main()
