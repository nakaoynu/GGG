# weighted_bayesian_fitting.py - é‡ã¿ä»˜ãå°¤åº¦é–¢æ•°ã‚’ç”¨ã„ãŸæ¸©åº¦ä¾å­˜ãƒ™ã‚¤ã‚ºæ¨å®š

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from pytensor.graph.op import Op
import os
import pathlib
import re
import warnings
import yaml
import datetime
from typing import List, Dict, Any, Tuple, Optional
from scipy.signal import find_peaks, peak_widths
from scipy.optimize import curve_fit

# æ•°å€¤è¨ˆç®—ã®è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore', category=RuntimeWarning)
np.seterr(all='ignore')  # NumPyã®è­¦å‘Šã‚‚æŠ‘åˆ¶

# --- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ©Ÿèƒ½ ---
def load_config(config_path: str = None) -> Dict[str, Any]:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«(YAML)ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ãƒãƒ¼ã‚¸ã™ã‚‹"""
    if config_path is None:
        config_path = pathlib.Path(__file__).parent / "config.yml"
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šå€¤
    default_config = {
        'file_paths': {
            'data_file': "C:\\Users\\taich\\OneDrive - YNU(ynu.jp)\\master\\ç£æ€§\\GGG\\Programs\\corrected_exp_datasets\\Corrected_Transmittance_Temperature.xlsx",
            'sheet_name': "Corrected Data",
            'results_parent_dir': "analysis_results"
        },
        'execution': {
            'use_gpu': True
        },
        'physical_parameters': {
            'B_fixed': 9.0,
            'd_fixed': 157.8e-6,
            's': 3.5,
            'N_spin': 1.9386e+28,
            'initial_values': {
                'eps_bg': 14.20,
                'g_factor': 2.003147,
                'B4': 0.000576,
                'B6': 0.000050,
                'gamma': 0.11e12,
                'a_scale': 0.604971
            }
        },
        'analysis_settings': {
            'temperature_columns': ['4K', '30K', '100K', '300K'],
            'low_freq_cutoff': 0.361505,
            'high_freq_cutoff': 0.45
        },
        'mcmc': {
            'draws': 3000,
            'tune': 2000,
            'chains': 4,
            'target_accept': 0.92
        }
    }
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            user_config = yaml.safe_load(f)
        
        # å†å¸°çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’æ›´æ–°
        def merge_dict(default, user):
            for key, value in user.items():
                if key in default and isinstance(default[key], dict) and isinstance(value, dict):
                    merge_dict(default[key], value)
                else:
                    default[key] = value
        
        merge_dict(default_config, user_config)
        print(f"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
        
    except FileNotFoundError:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« '{config_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    return default_config

# --- çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ ---
def create_results_directory(config: Dict[str, Any]) -> pathlib.Path:
    """å®Ÿè¡Œæ—¥æ™‚ã‚’å«ã‚€ä¸€æ„ã®çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    results_parent = config['file_paths']['results_parent_dir']
    results_dir = pathlib.Path(__file__).parent / results_parent / f"run_{timestamp}"
    results_dir.mkdir(parents=True, exist_ok=True)
    
    # ä½¿ç”¨ã—ãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ã‚³ãƒ”ãƒ¼
    config_backup_path = results_dir / "config_used.yml"
    with open(config_backup_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"ğŸ“ çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {results_dir.resolve()}")
    return results_dir

# --- 0. ç’°å¢ƒè¨­å®š ---
# ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã«æœ€åˆã«èª­ã¿è¾¼ã¾ã‚Œã¾ã™
print("--- 0. ç’°å¢ƒè¨­å®šã‚’é–‹å§‹ã—ã¾ã™ ---")

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
CONFIG = load_config()

# â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹3ã€‘GPUåˆ©ç”¨è¨­å®š â–¼â–¼â–¼
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GPUä½¿ç”¨ãƒ•ãƒ©ã‚°ã‚’å–å¾—
USE_GPU = CONFIG['execution']['use_gpu']
if USE_GPU:
    try:
        # cupyã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦GPUãŒåˆ©ç”¨å¯èƒ½ã‹ç°¡æ˜“çš„ã«åˆ¤æ–­
        import cupy
        print("âœ… GPU (CuPy) ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚GPUè¨­å®šã‚’è©¦ã¿ã¾ã™ã€‚")
        # PyTensorã®ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚’ 'cuda' ã«å¤‰æ›´
        os.environ['PYTENSOR_FLAGS'] = 'device=cuda,floatX=float64'
    except ImportError:
        print("âš ï¸ è­¦å‘Š: CuPyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚GPUã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
        os.environ['PYTENSOR_FLAGS'] = 'optimizer=fast_compile,floatX=float64'
else:
    print("ğŸ’» CPUè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    os.environ['PYTENSOR_FLAGS'] = 'optimizer=fast_compile,floatX=float64'

try:
    import japanize_matplotlib
except ImportError:
    print("è­¦å‘Š: japanize_matplotlib ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

plt.rcParams['figure.dpi'] = 120

# çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
RESULTS_DIR = create_results_directory(CONFIG)
print(f"ç”»åƒã¯ '{RESULTS_DIR.resolve()}' ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚")
# â–²â–²â–²ã€å¤‰æ›´ç‚¹3ã€‘GPUåˆ©ç”¨è¨­å®š â–²â–²â–²


# --- 1. ç‰©ç†å®šæ•°ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¡ä»¶ ---
# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å€¤ã‚’èª­ã¿è¾¼ã¿
print("--- 1. ç‰©ç†å®šæ•°ã¨åˆæœŸå€¤ã‚’è¨­å®šã—ã¾ã™ ---")
kB = 1.380649e-23; muB = 9.274010e-24; hbar = 1.054571e-34
c = 299792458; mu0 = 4.0 * np.pi * 1e-7

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
s = CONFIG['physical_parameters']['s']
N_spin = CONFIG['physical_parameters']['N_spin']
B_FIXED = CONFIG['physical_parameters']['B_fixed']
d_fixed = CONFIG['physical_parameters']['d_fixed']

# åˆæœŸå€¤ã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—
initial_values = CONFIG['physical_parameters']['initial_values']
eps_bg_init = initial_values['eps_bg']
B4_init = initial_values['B4']
B6_init = initial_values['B6']
gamma_init = initial_values['gamma']
a_scale_init = initial_values['a_scale']
g_factor_init = initial_values['g_factor']

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è¨­å®š
DATA_FILE_PATH = CONFIG['file_paths']['data_file']
DATA_SHEET_NAME = CONFIG['file_paths']['sheet_name']

# è§£æè¨­å®š
TEMPERATURE_COLUMNS = CONFIG['analysis_settings']['temperature_columns']
LOW_FREQUENCY_CUTOFF = CONFIG['analysis_settings']['low_freq_cutoff']
HIGH_FREQUENCY_CUTOFF = CONFIG['analysis_settings']['high_freq_cutoff']

# MCMCã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®š
MCMC_CONFIG = CONFIG['mcmc']


# --- ç‰©ç†ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•° ---
# å…ƒã®temperature_dependent_bayesian_fitting.pyã‹ã‚‰å¿…è¦ãªé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
class HamiltonianCache:
    """ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¨ˆç®—ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰"""
    def __init__(self):
        self._cache = {}
    
    def get_hamiltonian_cached(self, B_ext_z: float, g_factor: float, B4: float, B6: float) -> np.ndarray:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’å–å¾—ï¼ˆåŒä¸€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã®é‡è¤‡è¨ˆç®—ã‚’å›é¿ï¼‰"""
        key = (round(B_ext_z, 6), round(g_factor, 6), round(B4, 8), round(B6, 8))
        if key not in self._cache:
            self._cache[key] = get_hamiltonian(B_ext_z, g_factor, B4, B6)
        return self._cache[key]
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self._cache.clear()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_hamiltonian_cache = HamiltonianCache()

def get_hamiltonian_cached(B_ext_z: float, g_factor: float, B4: float, B6: float) -> np.ndarray:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¨ˆç®—ã®ãƒ‘ãƒ–ãƒªãƒƒã‚¯é–¢æ•°"""
    return _hamiltonian_cache.get_hamiltonian_cached(B_ext_z, g_factor, B4, B6)

# äº‹å‰è¨ˆç®—ã•ã‚ŒãŸå›ºå®šã‚¬ãƒ³ãƒé…åˆ—ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–ï¼‰
_FIXED_GAMMA_ARRAY = np.full(7, 0.11e12)

def normalize_gamma_array(gamma_input) -> np.ndarray:
    """ã‚¬ãƒ³ãƒé…åˆ—ã®æ­£è¦åŒ–ã¨å‹å®‰å…¨æ€§ã‚’ç¢ºä¿ï¼ˆã‚³ãƒ¼ãƒ‰é‡è¤‡å‰Šæ¸›ï¼‰"""
    if np.isscalar(gamma_input):
        return np.full(7, gamma_input)
    elif hasattr(gamma_input, 'ndim') and gamma_input.ndim == 0:
        return np.full(7, float(gamma_input))
    elif hasattr(gamma_input, '__len__'):
        if len(gamma_input) == 7:
            return np.array(gamma_input)
        elif len(gamma_input) > 7:
            return np.array(gamma_input[:7])
        else:
            return np.pad(np.array(gamma_input), (0, 7 - len(gamma_input)), 'edge')
    else:
        return np.full(7, gamma_input.item())

def get_eps_bg_initial_values_and_bounds(temperature: float) -> Tuple[List[float], Tuple[float, float]]:
    """æ¸©åº¦ä¾å­˜eps_bgåˆæœŸå€¤ã¨å¢ƒç•Œå€¤ã®å–å¾—ï¼ˆçµ±åˆãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ï¼‰"""
    eps_bg_init = 14.20
    if temperature <= 10:
        # ä½æ¸©ã§ã¯ä½ã‚ã®åˆæœŸå€¤ã‹ã‚‰é–‹å§‹ï¼ˆãƒ•ã‚©ãƒãƒ³ã®å‡çµåŠ¹æœï¼‰
        initial_eps_bg_values = [eps_bg_init * 0.85, eps_bg_init * 0.90, eps_bg_init * 0.95, eps_bg_init,
                                13.0, 12.5, 12.8, 13.2, 13.5, 14.0]
        bounds_eps_bg = (11.0, 16.0)
    elif temperature <= 100:
        # ä¸­é–“æ¸©åº¦ã§ã¯æ¨™æº–çš„ãªåˆæœŸå€¤
        initial_eps_bg_values = [eps_bg_init * 0.98, eps_bg_init, eps_bg_init * 1.02, eps_bg_init * 1.05,
                                13.8, 14.0, 14.2, 13.5, 14.5, 13.2]
        bounds_eps_bg = (11.5, 16.5)
    else:
        # é«˜æ¸©ã§ã¯é«˜ã‚ã®åˆæœŸå€¤ã‹ã‚‰é–‹å§‹ï¼ˆãƒ•ã‚©ãƒãƒ³ã®æ´»æ€§åŒ–ï¼‰
        initial_eps_bg_values = [eps_bg_init * 1.05, eps_bg_init * 1.10, eps_bg_init * 1.15, eps_bg_init,
                                14.5, 15.0, 15.5, 14.0, 16.0, 13.8]
        bounds_eps_bg = (12.0, 17.0)
    return initial_eps_bg_values, bounds_eps_bg

def get_hamiltonian(B_ext_z: float, g_factor: float, B4: float, B6: float) -> np.ndarray:
    """ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’è¨ˆç®—ã™ã‚‹"""
    m_values = np.arange(s, -s - 1, -1)
    Sz = np.diag(m_values)
    O40 = 60 * np.diag([7, -13, -3, 9, 9, -3, -13, 7])
    X_O44 = np.zeros((8, 8)); X_O44[3, 7], X_O44[4, 0] = np.sqrt(35), np.sqrt(35); X_O44[2, 6], X_O44[5, 1] = 5 * np.sqrt(3), 5 * np.sqrt(3)
    O44 = 12 * (X_O44 + X_O44.T)
    O60 = 1260 * np.diag([1, -5, 9, -5, -5, 9, -5, 1])
    X_O64 = np.zeros((8, 8)); X_O64[3, 7], X_O64[4, 0] = 3 * np.sqrt(35), 3 * np.sqrt(35); X_O64[2, 6], X_O64[5, 1] = -7 * np.sqrt(3), -7 * np.sqrt(3)
    O64 = 60 * (X_O64 + X_O64.T)
    H_cf = (B4 * kB) * (O40 + 5 * O44) + (B6 * kB) * (O60 - 21 * O64)
    H_zee = g_factor * muB * B_ext_z * Sz
    return H_cf + H_zee

def calculate_susceptibility(omega_array: np.ndarray, H: np.ndarray, T: float, gamma_array: np.ndarray) -> np.ndarray:
    """ç£æ°—æ„Ÿå—ç‡ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ¸©åº¦ä¾å­˜gammaå¯¾å¿œãƒ»å‹å®‰å…¨ç‰ˆï¼‰"""
    
    # çµ±åˆã•ã‚ŒãŸã‚¬ãƒ³ãƒé…åˆ—æ­£è¦åŒ–é–¢æ•°ã‚’ä½¿ç”¨ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
    gamma_array = normalize_gamma_array(gamma_array)

    eigenvalues, _ = np.linalg.eigh(H)
    eigenvalues -= np.min(eigenvalues)
    
    # æ•°å€¤çš„å®‰å®šæ€§ã®ãŸã‚ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
    eigenvalues = np.clip(eigenvalues / (kB * T), -700, 700)
    
    Z = np.sum(np.exp(-eigenvalues))
    populations = np.exp(-eigenvalues) / Z
    delta_E = (eigenvalues[1:] - eigenvalues[:-1]) * kB * T  # å…ƒã®å˜ä½ã«æˆ»ã™
    delta_pop = populations[1:] - populations[:-1]
    
    # ç„¡åŠ¹ãªå€¤ã‚’ãƒã‚§ãƒƒã‚¯
    valid_mask = np.isfinite(delta_E) & (np.abs(delta_E) > 1e-30)
    if not np.any(valid_mask):
        return np.zeros_like(omega_array, dtype=complex)
    
    omega_0 = delta_E / hbar
    m_vals = np.arange(s, -s, -1)
    transition_strength = (s + m_vals) * (s - m_vals + 1)
    
    # === ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›: gamma_arrayãŒdelta_Eã¨åŒã˜æ¬¡å…ƒã‚’æŒã¤ã‚ˆã†ã«èª¿æ•´ ===
    if len(gamma_array) != len(delta_E):
        if len(gamma_array) > len(delta_E):
            gamma_array = gamma_array[:len(delta_E)]
        else:
            gamma_array = np.pad(gamma_array, (0, len(delta_E) - len(gamma_array)), 'edge')
    
    # æ•°å€¤çš„å®‰å®šæ€§ã®å‘ä¸Š
    numerator = delta_pop * transition_strength
    
    # ç„¡åŠ¹ãªå€¤ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    finite_mask = np.isfinite(numerator) & np.isfinite(omega_0) & np.isfinite(gamma_array)
    numerator = numerator[finite_mask]
    omega_0_filtered = omega_0[finite_mask]
    gamma_filtered = gamma_array[finite_mask]
    
    if len(numerator) == 0:
        return np.zeros_like(omega_array, dtype=complex)
    
    # denominatorã®è¨ˆç®—ã‚’å®‰å…¨ã«å®Ÿè¡Œ
    chi_array = np.zeros_like(omega_array, dtype=complex)
    for i, omega in enumerate(omega_array):
        if not np.isfinite(omega):
            continue
        denominator = omega_0_filtered - omega - 1j * gamma_filtered
        # éå¸¸ã«å°ã•ã„å€¤ã‚’é¿ã‘ã‚‹
        denominator[np.abs(denominator) < 1e-20] = 1e-20 + 1j * 1e-20
        chi_array[i] = np.sum(numerator / denominator)
    
    return -chi_array

def calculate_normalized_transmission(omega_array: np.ndarray, mu_r_array: np.ndarray, d: float, eps_bg: float) -> np.ndarray:
    """æ­£è¦åŒ–é€éç‡ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆæ”¹è‰¯ç‰ˆï¼šæ•°å€¤å®‰å®šæ€§ã¨ãƒ”ãƒ¼ã‚¯ä½ç½®ç²¾åº¦ã®å‘ä¸Šï¼‰"""
    # å…¥åŠ›å€¤ã®æ¤œè¨¼ã¨å®‰å…¨ãªå‡¦ç†
    eps_bg = max(eps_bg, 0.1)  # æœ€å°å€¤ã‚’è¨­å®š
    d = max(d, 1e-6)  # æœ€å°å€¤ã‚’è¨­å®š
    
    # è¤‡ç´ å±ˆæŠ˜ç‡ã¨ impedance ã®è¨ˆç®—
    mu_r_safe = np.where(np.isfinite(mu_r_array), mu_r_array, 1.0)
    eps_mu_product = eps_bg * mu_r_safe
    eps_mu_product = np.where(eps_mu_product.real > 0, eps_mu_product, 0.1 + 1j * eps_mu_product.imag)
    
    n_complex = np.sqrt(eps_mu_product + 0j)
    impe = np.sqrt(mu_r_safe / eps_bg + 0j)
    
    # æ³¢é•·è¨ˆç®—ï¼ˆã‚¼ãƒ­å‘¨æ³¢æ•°ã‚’é¿ã‘ã‚‹ï¼‰
    lambda_0 = np.full_like(omega_array, np.inf, dtype=float)
    nonzero_mask = omega_array > 1e-12
    lambda_0[nonzero_mask] = (2 * np.pi * c) / omega_array[nonzero_mask]
    
    # ä½ç›¸è¨ˆç®—ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢ï¼‰
    delta = 2 * np.pi * n_complex * d / lambda_0
    delta = np.clip(delta.real, -700, 700) + 1j * np.clip(delta.imag, -700, 700)
    
    # é€éç‡è¨ˆç®—
    numerator = 4 * impe
    exp_pos = np.exp(-1j * delta)
    exp_neg = np.exp(1j * delta)
    
    denominator = (1 + impe)**2 * exp_pos - (1 - impe)**2 * exp_neg
    
    # åˆ†æ¯ãŒã‚¼ãƒ­ã«è¿‘ã„å ´åˆã®å‡¦ç†
    safe_mask = np.abs(denominator) > 1e-15
    t = np.zeros_like(denominator, dtype=complex)
    t[safe_mask] = numerator[safe_mask] / denominator[safe_mask]
    
    transmission = np.abs(t)**2
    
    # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚ã€ç•°å¸¸å€¤ã‚’é™¤å»
    transmission = np.where(np.isfinite(transmission), transmission, 0.0)
    transmission = np.clip(transmission, 0, 2)  # ç‰©ç†çš„ã«æ„å‘³ã®ã‚ã‚‹ç¯„å›²ã«åˆ¶é™
    
    # æ­£è¦åŒ–
    min_trans, max_trans = np.min(transmission), np.max(transmission)
    if max_trans > min_trans and np.isfinite(max_trans) and np.isfinite(min_trans):
        return (transmission - min_trans) / (max_trans - min_trans)
    else:
        return np.full_like(transmission, 0.5)

def load_data_full_range_temperature(file_path: str, sheet_name: str) -> List[Dict[str, Any]]:
    """å…¨å‘¨æ³¢æ•°ç¯„å›²ã®æ¸©åº¦ä¾å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=0)
    except Exception as e:
        raise FileNotFoundError(f"Excelãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“: {e}")
    
    freq_col = 'Frequency (THz)'
    df[freq_col] = pd.to_numeric(df[freq_col], errors='coerce')
    temp_cols = TEMPERATURE_COLUMNS
    
    all_datasets_full = []
    
    for col in temp_cols:
        if col not in df.columns:
            print(f"è­¦å‘Š: åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
            
        temp_value = float(col.replace('K', ''))
        df_clean = df[[freq_col, col]].dropna()
        freq, trans = df_clean[freq_col].values.astype(np.float64), df_clean[col].values.astype(np.float64)
        
        # å…¨ç¯„å›²ãƒ‡ãƒ¼ã‚¿
        all_datasets_full.append({
            'temperature': temp_value, 
            'b_field': B_FIXED, 
            'frequency': freq, 
            'transmittance_full': trans, 
            'omega': freq * 1e12 * 2 * np.pi
        })
    
    print(f"å…¨ç¯„å›²æ¸©åº¦ä¾å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(all_datasets_full)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    return all_datasets_full

def load_and_split_data_three_regions_temperature(file_path: str, sheet_name: str, 
                                                 low_cutoff: float = LOW_FREQUENCY_CUTOFF, 
                                                 high_cutoff: float = HIGH_FREQUENCY_CUTOFF) -> Dict[str, Any]:
    """æ¸©åº¦ä¾å­˜ãƒ‡ãƒ¼ã‚¿ã‚’1å›ã ã‘èª­ã¿è¾¼ã¿ã€ã™ã¹ã¦ã®å½¢å¼ã§æä¾›ã™ã‚‹çµ±ä¸€é–¢æ•°"""
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=0)
    except Exception as e:
        raise FileNotFoundError(f"Excelãƒ•ã‚¡ã‚¤ãƒ« '{file_path}' ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“: {e}")
    
    freq_col = 'Frequency (THz)'
    df[freq_col] = pd.to_numeric(df[freq_col], errors='coerce')
    temp_cols = TEMPERATURE_COLUMNS
    
    # å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’ä¸€åº¦ã«ä½œæˆ
    low_freq_datasets, mid_freq_datasets, high_freq_datasets = [], [], []
    all_datasets_full = []
    
    for col in temp_cols:
        if col not in df.columns:
            print(f"è­¦å‘Š: åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue
            
        temp_value = float(col.replace('K', ''))
        df_clean = df[[freq_col, col]].dropna()
        freq, trans = df_clean[freq_col].values.astype(np.float64), df_clean[col].values.astype(np.float64)
        
        # 3ã¤ã®é ˜åŸŸã«ãƒã‚¹ã‚¯ã‚’å®šç¾©
        low_mask = freq <= low_cutoff
        mid_mask = (freq > low_cutoff) & (freq < high_cutoff)
        high_mask = freq >= high_cutoff
        
        base_data = {'temperature': temp_value, 'b_field': B_FIXED}
        
        # ä½å‘¨æ³¢é ˜åŸŸ
        if np.any(low_mask):
            min_low, max_low = trans[low_mask].min(), trans[low_mask].max()
            trans_norm_low = (trans[low_mask] - min_low) / (max_low - min_low) if max_low > min_low else np.full_like(trans[low_mask], 0.5)
            low_freq_datasets.append({**base_data, 'frequency': freq[low_mask], 'transmittance': trans_norm_low, 'omega': freq[low_mask] * 1e12 * 2 * np.pi})
        
        # ä¸­é–“é ˜åŸŸ
        if np.any(mid_mask):
            min_mid, max_mid = trans[mid_mask].min(), trans[mid_mask].max()
            trans_norm_mid = (trans[mid_mask] - min_mid) / (max_mid - min_mid) if max_mid > min_mid else np.full_like(trans[mid_mask], 0.5)
            mid_freq_datasets.append({**base_data, 'frequency': freq[mid_mask], 'transmittance': trans_norm_mid, 'omega': freq[mid_mask] * 1e12 * 2 * np.pi})
        
        # é«˜å‘¨æ³¢é ˜åŸŸ
        if np.any(high_mask):
            min_high, max_high = trans[high_mask].min(), trans[high_mask].max()
            trans_norm_high = (trans[high_mask] - min_high) / (max_high - min_high) if max_high > min_high else np.full_like(trans[high_mask], 0.5)
            high_freq_datasets.append({**base_data, 'frequency': freq[high_mask], 'transmittance': trans_norm_high, 'omega': freq[high_mask] * 1e12 * 2 * np.pi})
        
        # å…¨ç¯„å›²ãƒ‡ãƒ¼ã‚¿
        all_datasets_full.append({**base_data, 'frequency': freq, 'transmittance_full': trans, 'omega': freq * 1e12 * 2 * np.pi})
    
    print(f"æ¸©åº¦ä¾å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†:")
    print(f"  ä½å‘¨æ³¢é ˜åŸŸ [~, {low_cutoff}THz]: {len(low_freq_datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    print(f"  ä¸­é–“é ˜åŸŸ [{low_cutoff}THz, {high_cutoff}THz]: {len(mid_freq_datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    print(f"  é«˜å‘¨æ³¢é ˜åŸŸ [{high_cutoff}THz, ~]: {len(high_freq_datasets)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    print(f"  å…¨ç¯„å›²ãƒ‡ãƒ¼ã‚¿: {len(all_datasets_full)} ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")
    
    return {
        'low_freq': low_freq_datasets, 
        'mid_freq': mid_freq_datasets,
        'high_freq': high_freq_datasets,
        'all_full': all_datasets_full
    }

def fit_eps_bg_only_temperature(dataset: Dict[str, Any], 
                               fixed_params: Optional[Dict[str, Any]] = None,
                               G0_from_bayesian: Optional[float] = None) -> Dict[str, float]:
    """å„æ¸©åº¦ã§é«˜å‘¨æ³¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰eps_bgã®ã¿ã‚’ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ï¼ˆä»–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å›ºå®šï¼‰"""
    print(f"\n--- æ¸©åº¦ {dataset['temperature']} K ã®é«˜å‘¨æ³¢eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° ---")
    
    # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
    if fixed_params is None:
        fixed_params = {
            'd': d_fixed,
            'g_factor': g_factor_init,
            'B4': B4_init,
            'B6': B6_init,
            'gamma_fixed': 0.11e12  # é«˜å‘¨æ³¢é ˜åŸŸã§ã¯å˜ä¸€ã®å›ºå®šå€¤ã‚’ä½¿ç”¨
        }
    
    def magnetic_cavity_model_eps_bg_only(freq_thz, eps_bg_fit):
        """eps_bgã®ã¿ã‚’å¤‰æ•°ã¨ã™ã‚‹é«˜å‘¨æ³¢é€éç‡ãƒ¢ãƒ‡ãƒ«ï¼ˆÎ³ã¯ç‰©ç†çš„ã«å¦¥å½“ãªå˜ä¸€å€¤ã§å›ºå®šï¼‰"""
        try:
            omega = freq_thz * 1e12 * 2 * np.pi
            
            # å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å€¤ã‚’å–å¾—
            g_factor_fit = fixed_params['g_factor']
            B4_fit = fixed_params['B4']
            B6_fit = fixed_params['B6']
            gamma_fixed = fixed_params['gamma_fixed']  # å˜ä¸€ã®å›ºå®šÎ³å€¤
            
            # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã¨ç£æ°—æ„Ÿå—ç‡ã®è¨ˆç®—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
            H = get_hamiltonian_cached(B_FIXED, g_factor_fit, B4_fit, B6_fit)
            
            # é«˜å‘¨æ³¢é ˜åŸŸã§ã¯å˜ä¸€ã®Î³å€¤ã‚’7è¦ç´ ã«è¤‡è£½ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ç‰ˆï¼‰
            gamma_array = _FIXED_GAMMA_ARRAY  # äº‹å‰è¨ˆç®—ã•ã‚ŒãŸé…åˆ—ã‚’ä½¿ç”¨
            chi_raw = calculate_susceptibility(omega, H, dataset['temperature'], gamma_array)
            
            # ç£æ°—æ„Ÿå—ç‡ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            G0 = mu0 * N_spin * (g_factor_fit * muB)**2 / (2 * hbar)
            chi = G0 * chi_raw
            
            # H_formã§é€ç£ç‡ã‚’è¨ˆç®—
            mu_r = 1 + chi
            
            # d_fixedã‚’ç›´æ¥ä½¿ç”¨
            return calculate_normalized_transmission(omega, mu_r, d_fixed, eps_bg_fit)
        except Exception as e:
            print(f"    è­¦å‘Š: ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {e}")
            return np.ones_like(freq_thz) * 0.5

    # è¤‡æ•°ã®åˆæœŸå€¤ã‚’è©¦è¡Œ
    success = False
    result = {}
    
    # æ¸©åº¦ä¾å­˜ã®åˆæœŸå€¤ã¨å¢ƒç•Œå€¤ã‚’å–å¾—ï¼ˆçµ±åˆé–¢æ•°ä½¿ç”¨ï¼‰
    initial_eps_bg_values, bounds_eps_bg = get_eps_bg_initial_values_and_bounds(dataset['temperature'])
    
    for attempt, initial_eps_bg in enumerate(initial_eps_bg_values):
        try:
            print(f"  è©¦è¡Œ {attempt + 1}: eps_bgåˆæœŸå€¤ = {initial_eps_bg:.3f}")
            
            popt, pcov = curve_fit(
                magnetic_cavity_model_eps_bg_only,
                dataset['frequency'],
                dataset['transmittance'],
                p0=[initial_eps_bg],
                bounds=([bounds_eps_bg[0]], [bounds_eps_bg[1]]),
                maxfev=3000,
                method='trf'
            )
            
            eps_bg_fit = popt[0]
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒç‰©ç†çš„ã«å¦¥å½“ã‹ãƒã‚§ãƒƒã‚¯
            if bounds_eps_bg[0] <= eps_bg_fit <= bounds_eps_bg[1]:
                print(f"  æˆåŠŸ (è©¦è¡Œ {attempt + 1}): eps_bg = {eps_bg_fit:.3f}")
                result = {
                    'eps_bg': eps_bg_fit,
                    'd': d_fixed,  # å›ºå®šå€¤ã‚’ç›´æ¥ä½¿ç”¨
                    'temperature': dataset['temperature']
                }
                success = True
                break
            else:
                print(f"  å¤±æ•— (è©¦è¡Œ {attempt + 1}): eps_bg = {eps_bg_fit:.3f} ã¯ç¯„å›²å¤–")
                
        except RuntimeError as e:
            print(f"  å¤±æ•— (è©¦è¡Œ {attempt + 1}): æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼ - {e}")
        except Exception as e:
            print(f"  å¤±æ•— (è©¦è¡Œ {attempt + 1}): ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ - {e}")
    
    if not success:
        print("  âŒ å…¨ã¦ã®è©¦è¡Œã«å¤±æ•—")
        result = {}
    
    return result


# â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹1ã€‘å°¤åº¦é‡ã¿ä»˜ã‘é–¢æ•°ã®è¿½åŠ  â–¼â–¼â–¼
def create_frequency_weights(dataset: Dict[str, Any]) -> np.ndarray:
    """
    å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ãƒ”ãƒ¼ã‚¯ç‰¹æ€§ã«åŸºã¥ãã€å°¤åº¦é–¢æ•°ã®ãŸã‚ã®é‡ã¿é…åˆ—ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    - LP, UP, é«˜å‘¨æ³¢ãƒ¢ãƒ¼ãƒ‰ã®åŠå€¤å¹…é ˜åŸŸ: é‡ã¿ 1.0
    - LP ã¨ UP ã®é–“ã®é ˜åŸŸ: é‡ã¿ 0.1
    - ãã®ä»–ã®é ˜åŸŸ: é‡ã¿ 0.0 (ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‹ã‚‰é™¤å¤–)
    """
    freq = dataset['frequency']
    trans = dataset['transmittance_full']
    
    # å¸åã‚¹ãƒšã‚¯ãƒˆãƒ«ã«å¤‰æ›ã—ã¦ãƒ”ãƒ¼ã‚¯ã‚’æ¤œå‡ºã—ã‚„ã™ãã™ã‚‹
    absorption = 1 - (trans / np.max(trans))
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
    peaks, properties = find_peaks(absorption, height=0.05, prominence=0.05, distance=10)
    
    if len(peaks) < 2:
        # ãƒ”ãƒ¼ã‚¯ãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã€ä½å‘¨æ³¢é ˜åŸŸå…¨ä½“ã«å‡ä¸€ãªé‡ã¿ã‚’ä»˜ã‘ã‚‹
        weights = np.zeros_like(freq)
        low_freq_mask = freq < HIGH_FREQUENCY_CUTOFF
        weights[low_freq_mask] = 1.0
        return weights

    # åŠå€¤å¹…ã‚’è¨ˆç®—
    widths, _, left_ips, right_ips = peak_widths(absorption, peaks, rel_height=0.5)
    
    # å‘¨æ³¢æ•°å˜ä½ã«å¤‰æ›
    left_freq = np.interp(left_ips, np.arange(len(freq)), freq)
    right_freq = np.interp(right_ips, np.arange(len(freq)), freq)

    # é‡ã¿é…åˆ—ã®åˆæœŸåŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0)
    weights = np.zeros_like(freq)

    # LPã¨UPã‚’ç‰¹å®š (ä½å‘¨æ³¢å´ã®2ã¤ã®ä¸»è¦ãªãƒ”ãƒ¼ã‚¯ã¨ä»®å®š)
    low_freq_peaks = peaks[freq[peaks] < HIGH_FREQUENCY_CUTOFF]
    if len(low_freq_peaks) >= 2:
        # ãƒ—ãƒ­ãƒŸãƒãƒ³ã‚¹ã§ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½2ã¤ã‚’å–å¾—
        peak_prominences = properties['prominences'][freq[peaks] < HIGH_FREQUENCY_CUTOFF]
        sorted_indices = np.argsort(peak_prominences)[::-1]
        
        lp_idx_in_all_peaks = np.where(peaks == low_freq_peaks[sorted_indices[1]])[0][0]
        up_idx_in_all_peaks = np.where(peaks == low_freq_peaks[sorted_indices[0]])[0][0]
        
        lp_freq_peak = freq[peaks[lp_idx_in_all_peaks]]
        up_freq_peak = freq[peaks[up_idx_in_all_peaks]]
        
        # LP-UPé–“ã«é‡ã¿0.1ã‚’ä»˜ä¸
        between_mask = (freq >= lp_freq_peak) & (freq <= up_freq_peak)
        weights[between_mask] = 0.1
        
        # LPã¨UPã®åŠå€¤å¹…é ˜åŸŸã«é‡ã¿1.0ã‚’ä»˜ä¸
        lp_fwhm_mask = (freq >= left_freq[lp_idx_in_all_peaks]) & (freq <= right_freq[lp_idx_in_all_peaks])
        up_fwhm_mask = (freq >= left_freq[up_idx_in_all_peaks]) & (freq <= right_freq[up_idx_in_all_peaks])
        weights[lp_fwhm_mask] = 1.0
        weights[up_fwhm_mask] = 1.0
        
    # é«˜å‘¨æ³¢ã®å…±æŒ¯å™¨ãƒ¢ãƒ¼ãƒ‰ã«ã‚‚é‡ã¿1.0ã‚’ä»˜ä¸
    high_freq_peak_indices = np.where(freq[peaks] >= HIGH_FREQUENCY_CUTOFF)[0]
    for idx_in_all_peaks in high_freq_peak_indices:
        fwhm_mask = (freq >= left_freq[idx_in_all_peaks]) & (freq <= right_freq[idx_in_all_peaks])
        weights[fwhm_mask] = 1.0

    print(f"  æ¸©åº¦ {dataset['temperature']}K: é‡ã¿é…åˆ—ã‚’ç”Ÿæˆã€‚é‡ã¿>0ã®ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {np.sum(weights > 0)} / {len(freq)}")
    return weights
# â–²â–²â–²ã€å¤‰æ›´ç‚¹1ã€‘å°¤åº¦é‡ã¿ä»˜ã‘é–¢æ•°ã®è¿½åŠ  â–²â–²â–²


class TemperatureMagneticModelOp(Op):
    """æ¸©åº¦ä¾å­˜ã®ä½å‘¨æ³¢é ˜åŸŸã®ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¨å®šã™ã‚‹ãŸã‚ã®PyMC Opï¼ˆæ¸©åº¦ä¾å­˜gammaå¯¾å¿œï¼‰ã€‚"""
    def __init__(self, datasets: List[Dict[str, Any]], temperature_specific_params: Dict[float, Dict[str, float]], model_type: str):
        super().__init__()
        self.datasets = datasets
        self.temperature_specific_params = temperature_specific_params
        self.model_type = model_type
        self.temp_list = sorted(list(set([d['temperature'] for d in datasets])))
        # æ¸©åº¦ä¾å­˜gammaã«å¯¾å¿œã™ã‚‹ãŸã‚inputã‚¿ã‚¤ãƒ—ã‚’æ‹¡å¼µ
        self.itypes = [pt.dscalar, pt.dvector, pt.dscalar, pt.dscalar, pt.dscalar]  # a_scale, gamma_concat, g_factor, B4, B6
        self.otypes = [pt.dvector]
    
    def perform(self, node, inputs, output_storage):
        a_scale, gamma_concat, g_factor, B4, B6 = inputs
        full_predicted_y = []
        gamma_start_idx = 0
        
        for data in self.datasets:
            # è©²å½“ã™ã‚‹æ¸©åº¦ã®å›ºå®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            temperature = data['temperature']
            if temperature in self.temperature_specific_params:
                d_fixed = self.temperature_specific_params[temperature]['d']
                eps_bg_fixed = self.temperature_specific_params[temperature]['eps_bg']
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                d_fixed = globals()['d_fixed']
                eps_bg_fixed = eps_bg_init
            
            # æ¸©åº¦ä¾å­˜gammaã®å–å¾—ï¼ˆ7å€‹ãšã¤åˆ†å‰²ï¼‰
            gamma_end_idx = gamma_start_idx + 7
            gamma_for_temp = gamma_concat[gamma_start_idx:gamma_end_idx]
            gamma_start_idx = gamma_end_idx
            
            # ç‰©ç†ãƒ¢ãƒ‡ãƒ«è¨ˆç®—
            H = get_hamiltonian(B_FIXED, g_factor, B4, B6)
            chi_raw = calculate_susceptibility(data['omega'], H, temperature, gamma_for_temp)
            
            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã®è¨ˆç®—
            G0 = a_scale * mu0 * N_spin * (g_factor * muB)**2 / (2 * hbar)
            chi = G0 * chi_raw
            
            # ãƒ¢ãƒ‡ãƒ«å½¢å¼ã«å¿œã˜ãŸé€ç£ç‡è¨ˆç®—
            if self.model_type == 'B_form':
                mu_r = 1 / (1 - chi)
            else:  # H_form (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
                mu_r = 1 + chi
            
            # é€éç‡è¨ˆç®—
            predicted_trans = calculate_normalized_transmission(data['omega'], mu_r, d_fixed, eps_bg_fixed)
            
            # æ•°å€¤çš„å®‰å®šæ€§ã®ãƒã‚§ãƒƒã‚¯
            predicted_trans = np.where(np.isfinite(predicted_trans), predicted_trans, 0.5)
            predicted_trans = np.clip(predicted_trans, 0, 1)
            
            full_predicted_y.extend(predicted_trans)
        
        output_storage[0][0] = np.array(full_predicted_y)

def extract_bayesian_parameters(trace: az.InferenceData) -> Dict[str, float]:
    """ãƒ™ã‚¤ã‚ºæ¨å®šçµæœã‹ã‚‰å¹³å‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆæ¸©åº¦ä¾å­˜gammaå¯¾å¿œï¼‰"""
    posterior = trace["posterior"]
    a_scale_mean = posterior['a_scale'].mean().item()
    g_factor_mean = posterior['g_factor'].mean().item()
    result = {
        'a_scale': a_scale_mean,
        'g_factor': g_factor_mean,
        'B4': posterior['B4'].mean().item(),
        'B6': posterior['B6'].mean().item(),
        'G0': a_scale_mean * mu0 * N_spin * (g_factor_mean * muB)**2 / (2 * hbar)
    }
    
    # æ¸©åº¦ä¾å­˜gammaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚è¿½åŠ 
    try:
        result['log_gamma_mu_base'] = posterior['log_gamma_mu_base'].mean().item()
        result['temp_gamma_slope'] = posterior['temp_gamma_slope'].mean().item()
        result['temp_gamma_nonlinear'] = posterior['temp_gamma_nonlinear'].mean().item()
    except KeyError:
        # æ¸©åº¦ä¾å­˜gammaãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        pass
    
    return result


def run_temperature_bayesian_fit(datasets: List[Dict[str, Any]], 
                                temperature_specific_params: Dict[float, Dict[str, float]],
                                # â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹1ã€‘é‡ã¿é…åˆ—ã‚’å¼•æ•°ã«è¿½åŠ  â–¼â–¼â–¼
                                weights_list: List[np.ndarray], 
                                # â–²â–²â–²ã€å¤‰æ›´ç‚¹1ã€‘é‡ã¿é…åˆ—ã‚’å¼•æ•°ã«è¿½åŠ  â–²â–²â–²
                                prior_magnetic_params: Optional[Dict[str, float]] = None, 
                                model_type: str = 'H_form') -> Optional[az.InferenceData]:
    print(f"\n--- é‡ã¿ä»˜ããƒ™ã‚¤ã‚ºæ¨å®š (ãƒ¢ãƒ‡ãƒ«: {model_type}) ---")
    
    # è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã¨é‡ã¿ã‚’çµåˆ
    trans_observed = np.concatenate([d['transmittance_full'] for d in datasets])
    combined_weights = np.concatenate(weights_list)

    with pm.Model() as model:
        # äº‹å‰åˆ†å¸ƒã®è¨­å®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆæœŸå€¤ã‚’å–å¾—ï¼‰
        if prior_magnetic_params is None:
            # åˆå›å®Ÿè¡Œæ™‚ã¯è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸå€¤ã‚’ä½¿ç”¨
            a_scale = pm.Normal('a_scale', mu=a_scale_init, sigma=0.3)
            g_factor = pm.Normal('g_factor', mu=g_factor_init, sigma=0.2)
            B4 = pm.Normal('B4', mu=B4_init, sigma=0.0002)
            B6 = pm.Normal('B6', mu=B6_init, sigma=0.00002)
        else:
            # äº‹å‰æƒ…å ±ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            a_scale = pm.Normal('a_scale', mu=prior_magnetic_params['a_scale'], sigma=0.2)
            g_factor = pm.Normal('g_factor', mu=prior_magnetic_params['g_factor'], sigma=0.1)
            B4 = pm.Normal('B4', mu=prior_magnetic_params['B4'], sigma=0.0001)
            B6 = pm.Normal('B6', mu=prior_magnetic_params['B6'], sigma=0.00001)
        
        # æ¸©åº¦ä¾å­˜gammaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        log_gamma_mu_base = pm.Normal('log_gamma_mu_base', mu=np.log(gamma_init), sigma=1.0)
        temp_gamma_slope = pm.Normal('temp_gamma_slope', mu=0.0, sigma=0.01)
        temp_gamma_nonlinear = pm.Normal('temp_gamma_nonlinear', mu=0.0, sigma=0.001)
        log_gamma_sigma_base = pm.HalfNormal('log_gamma_sigma_base', sigma=1.0)
        log_gamma_offset_base = pm.Normal('log_gamma_offset_base', mu=0.0, sigma=1.0)
        
        # å„æ¸©åº¦ã®å„é·ç§»ã®gammaã‚’ç”Ÿæˆ
        gamma_list = []
        for data in datasets:
            T = data['temperature']
            log_gamma_mu_temp = log_gamma_mu_base + temp_gamma_slope * T + temp_gamma_nonlinear * T**2
            gamma_temp = pm.LogNormal(f'gamma_T{T}', mu=log_gamma_mu_temp + log_gamma_offset_base, sigma=log_gamma_sigma_base, shape=7)
            gamma_list.append(gamma_temp)
        
        gamma_concat = pt.concatenate(gamma_list)
        
        op = TemperatureMagneticModelOp(datasets, temperature_specific_params, model_type)
        mu = op(a_scale, gamma_concat, g_factor, B4, B6)
        
        # â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹1ã€‘é‡ã¿ä»˜ãå°¤åº¦é–¢æ•°ã®å®Ÿè£… â–¼â–¼â–¼
        # é‡ã¿ãŒ0ã‚ˆã‚Šå¤§ãã„ãƒ‡ãƒ¼ã‚¿ç‚¹ã®ã¿ã‚’ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹
        valid_indices = np.where(combined_weights > 0)[0]
        
        # é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å†æ§‹ç¯‰
        trans_observed_weighted = trans_observed[valid_indices]
        weights_weighted = combined_weights[valid_indices]
        
        # é‡ã¿ä»˜ããƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã™ã‚‹ãŸã‚ã®æ–°ã—ã„Opã‚’ä½œæˆ
        datasets_weighted = []
        weights_start_idx = 0
        for i, data in enumerate(datasets):
            n_points = len(data['transmittance_full'])
            weights_end_idx = weights_start_idx + n_points
            
            # ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é‡ã¿
            dataset_weights = combined_weights[weights_start_idx:weights_end_idx]
            dataset_valid_indices = np.where(dataset_weights > 0)[0]
            
            if len(dataset_valid_indices) > 0:
                # é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
                weighted_dataset = {
                    'temperature': data['temperature'],
                    'b_field': data['b_field'],
                    'frequency': data['frequency'][dataset_valid_indices],
                    'transmittance_full': data['transmittance_full'][dataset_valid_indices],
                    'omega': data['omega'][dataset_valid_indices],
                    'weights': dataset_weights[dataset_valid_indices]
                }
                datasets_weighted.append(weighted_dataset)
            
            weights_start_idx = weights_end_idx
        
        # é‡ã¿ä»˜ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®Opã‚’ä½¿ç”¨
        if datasets_weighted:
            op_weighted = TemperatureMagneticModelOp(datasets_weighted, temperature_specific_params, model_type)
            mu = op_weighted(a_scale, gamma_concat, g_factor, B4, B6)
            
            # é‡ã¿é…åˆ—ã‚’çµ±åˆ
            weights_tensor = pt.as_tensor_variable(np.concatenate([d['weights'] for d in datasets_weighted]))
            
            sigma = pm.HalfNormal('sigma', sigma=0.05)
            
            # é‡ã¿ã«å¿œã˜ã¦sigmaã‚’èª¿æ•´ (é‡ã¿ãŒå¤§ãã„ã»ã©sigmaã¯å°ã•ããªã‚‹)
            sigma_adjusted = sigma / pt.sqrt(weights_tensor)
            
            # é‡ã¿ä»˜ããƒ‡ãƒ¼ã‚¿ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
            trans_target = np.concatenate([d['transmittance_full'] for d in datasets_weighted])
            
            # é‡ã¿ä»˜ã‘ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§å°¤åº¦ã‚’è¨ˆç®—
            Y_obs = pm.Normal('Y_obs', mu=mu, sigma=sigma_adjusted, observed=trans_target)
        else:
            print("âš ï¸ è­¦å‘Š: æœ‰åŠ¹ãªé‡ã¿ä»˜ããƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return None
        # â–²â–²â–²ã€å¤‰æ›´ç‚¹1ã€‘é‡ã¿ä»˜ãå°¤åº¦é–¢æ•°ã®å®Ÿè£… â–²â–²â–²
        
        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°è¨­å®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        try:
            trace = pm.sample(
                draws=MCMC_CONFIG['draws'], 
                tune=MCMC_CONFIG['tune'], 
                chains=MCMC_CONFIG['chains'],
                target_accept=MCMC_CONFIG['target_accept'],
                return_inferencedata=True
            )
            print("âœ… ãƒ™ã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ ãƒ™ã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None

    # ãƒ™ã‚¤ã‚ºæ¨å®šçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    trace_filename = RESULTS_DIR / f'trace_{model_type}.nc'
    az.to_netcdf(trace, trace_filename)
    print(f"âœ… Traceã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {trace_filename}")

    print("----------------------------------------------------")
    print("â–¶ æ¸©åº¦ä¾å­˜ãƒ™ã‚¤ã‚ºæ¨å®šçµæœ (ã‚µãƒãƒªãƒ¼):")
    try:
        summary = az.summary(trace, var_names=['a_scale', 'g_factor', 'B4', 'B6'])
        print(summary)
    except KeyError as e:
        print(f"ã‚µãƒãƒªãƒ¼è¡¨ç¤ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        try:
            summary = az.summary(trace)
            print(summary)
        except:
            print("ã‚µãƒãƒªãƒ¼ã®è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    print("----------------------------------------------------")
    return trace


# â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹2ã€‘äº‹å¾Œåˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°ã®è¿½åŠ  â–¼â–¼â–¼
def plot_posterior_distributions(trace: az.InferenceData, model_type: str):
    """
    ãƒ™ã‚¤ã‚ºæ¨å®šã®äº‹å¾Œåˆ†å¸ƒã¨ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã‚’å¯è¦–åŒ–ã—ã€ä¿å­˜ã™ã‚‹ã€‚
    ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®åæŸã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒå½¢çŠ¶ã‚’ç¢ºèªã§ãã‚‹ã€‚
    """
    print(f"\n--- {model_type}ãƒ¢ãƒ‡ãƒ«ã®äº‹å¾Œåˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆä¸­ ---")
    
    # ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ä¸»è¦ãªå¤‰æ•°ã‚’æŒ‡å®š
    var_names = ['a_scale', 'g_factor', 'B4', 'B6', 
                 'log_gamma_mu_base', 'temp_gamma_slope', 'temp_gamma_nonlinear']
    
    try:
        # ArviZã®plot_traceã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
        axes = az.plot_trace(trace, var_names=var_names, compact=True, kind='rank_bars')
        plt.suptitle(f'Posterior Trace Plot for {model_type} Model', fontsize=16, y=1.02)
        fig = plt.gcf() # ç¾åœ¨ã®Figureã‚’å–å¾—
        fig.savefig(RESULTS_DIR / f'posterior_trace_{model_type}.png', bbox_inches='tight')
        plt.show()
        print(f"âœ… äº‹å¾Œåˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: posterior_trace_{model_type}.png")

    except Exception as e:
        print(f"âš ï¸ äº‹å¾Œåˆ†å¸ƒã®ãƒ—ãƒ­ãƒƒãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
# â–²â–²â–²ã€å¤‰æ›´ç‚¹2ã€‘äº‹å¾Œåˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆé–¢æ•°ã®è¿½åŠ  â–²â–²â–²


def run_analysis_workflow():
    """
    ã€å¤‰æ›´ç‚¹1ã€‘åå¾©ãƒ—ãƒ­ã‚»ã‚¹ã‚’å»ƒæ­¢ã—ã€é‡ã¿ä»˜ã‘ã‚’åˆ©ç”¨ã—ãŸå˜ä¸€ã®è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€‚
    1. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€é«˜å‘¨æ³¢ã¨å…¨å‘¨æ³¢æ•°é ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã€‚
    2. é«˜å‘¨æ³¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„æ¸©åº¦ã®eps_bgã‚’ä¸€åº¦ã ã‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã€‚
    3. å…¨å‘¨æ³¢æ•°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’åŸºã«å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ã‚’ç”Ÿæˆã€‚
    4. é‡ã¿ä»˜ã‘ã—ãŸå°¤åº¦é–¢æ•°ã‚’ç”¨ã„ã¦ãƒ™ã‚¤ã‚ºæ¨å®šã‚’å®Ÿè¡Œã€‚
    5. çµæœã‚’å¯è¦–åŒ–ãƒ»ä¿å­˜ã™ã‚‹ã€‚
    """
    print("ğŸš€ é‡ã¿ä»˜ããƒ™ã‚¤ã‚ºæ¨å®šãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    all_datasets_full_range = load_data_full_range_temperature(DATA_FILE_PATH, DATA_SHEET_NAME)
    high_freq_datasets = load_and_split_data_three_regions_temperature(
        file_path=DATA_FILE_PATH, sheet_name=DATA_SHEET_NAME
    )['high_freq']

    if not all_datasets_full_range or not high_freq_datasets:
        print("âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
        
    # 2. å„æ¸©åº¦ã®eps_bgã‚’ä¸€åº¦ã ã‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    print("\n--- ã‚¹ãƒ†ãƒƒãƒ—1: å„æ¸©åº¦ã®é«˜å‘¨æ³¢eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° ---")
    temperature_specific_params = {}
    for dataset in high_freq_datasets:
        temp = dataset['temperature']
def fit_single_temperature_cavity_modes(dataset: Dict[str, Any]) -> Dict[str, float]:
    """é«˜å‘¨æ³¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆfit_eps_bg_only_temperatureã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
    return fit_eps_bg_only_temperature(dataset)

def save_fitting_parameters_to_csv(final_traces: Dict[str, az.InferenceData], 
                                  temperature_specific_params: Dict[float, Dict[str, float]]):
    """ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    print("\n--- ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’CSVã«ä¿å­˜ä¸­ ---")
    
    # ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµæœã‚’ä¿å­˜
    for model_type, trace in final_traces.items():
        params = extract_bayesian_parameters(trace)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµæœ
        params_df = pd.DataFrame([params])
        params_file = RESULTS_DIR / f'fitting_parameters_{model_type}.csv'
        params_df.to_csv(params_file, index=False)
        print(f"âœ… ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {params_file}")
    
    # æ¸©åº¦åˆ¥å…‰å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµæœã‚’ä¿å­˜
    temp_params_list = []
    for temp, params in sorted(temperature_specific_params.items()):
        temp_params_list.append(params)
    
    if temp_params_list:
        temp_df = pd.DataFrame(temp_params_list)
        temp_file = RESULTS_DIR / 'temperature_optical_parameters.csv'
        temp_df.to_csv(temp_file, index=False)
        print(f"âœ… æ¸©åº¦åˆ¥å…‰å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {temp_file}")

def plot_temperature_dependencies(temperature_specific_params: Dict[float, Dict[str, float]], 
                                trace: az.InferenceData):
    """æ¸©åº¦ä¾å­˜æ€§ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ï¼ˆè†œåšã¯å›ºå®šå€¤ã®ãŸã‚é™¤å¤–ï¼‰"""
    print("\n--- æ¸©åº¦ä¾å­˜æ€§ã®å¯è¦–åŒ– ---")
    
    temperatures = sorted(temperature_specific_params.keys())
    eps_bg_values = [temperature_specific_params[T]['eps_bg'] for T in temperatures]
    
    # ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    magnetic_params = extract_bayesian_parameters(trace)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # eps_bg ã®æ¸©åº¦ä¾å­˜æ€§
    ax1.plot(temperatures, eps_bg_values, 'ro-', linewidth=2, markersize=8, label='èƒŒæ™¯èª˜é›»ç‡')
    ax1.set_xlabel('æ¸©åº¦ (K)', fontsize=12)
    ax1.set_ylabel('èƒŒæ™¯èª˜é›»ç‡ eps_bg', fontsize=12)
    ax1.set_title('èƒŒæ™¯èª˜é›»ç‡ã®æ¸©åº¦ä¾å­˜æ€§', fontsize=14)
    ax1.grid(True, linestyle='--', alpha=0.6)
    ax1.legend()
    
    # å€¤ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º
    for T, eps in zip(temperatures, eps_bg_values):
        ax1.annotate(f'{eps:.2f}', (T, eps), textcoords="offset points", xytext=(0,10), ha='center')
    
    # æ¸©åº¦ã«ã‚ˆã‚‹åŠ¹æœã®æ¦‚è¦ï¼ˆã‚µãƒãƒªãƒ¼ãƒ‘ãƒãƒ«ï¼‰
    ax2.text(0.1, 0.9, f'æ¸©åº¦ç¯„å›²: {min(temperatures)} - {max(temperatures)} K', fontsize=14, transform=ax2.transAxes)
    ax2.text(0.1, 0.8, f'eps_bgå¤‰åŒ–ç‡: {(max(eps_bg_values)-min(eps_bg_values))/min(eps_bg_values)*100:.1f}%', fontsize=14, transform=ax2.transAxes)
    ax2.text(0.1, 0.7, f'å›ºå®šç£å ´: {B_FIXED} T', fontsize=14, transform=ax2.transAxes)
    
    ax2.text(0.1, 0.5, 'ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (æ¸©åº¦éä¾å­˜):', fontsize=14, transform=ax2.transAxes, weight='bold')
    ax2.text(0.1, 0.4, f'gå› å­ = {magnetic_params["g_factor"]:.4f}', fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.3, f'B4 = {magnetic_params["B4"]:.6f} K', fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.2, f'B6 = {magnetic_params["B6"]:.6f} K', fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.1, f'G0 = {magnetic_params["G0"]:.3e}', fontsize=12, transform=ax2.transAxes)
    
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.set_title('è§£æçµæœã‚µãƒãƒªãƒ¼', fontsize=14)
    ax2.axis('off')
    
    plt.tight_layout()
    plt.savefig(RESULTS_DIR / 'temperature_dependencies.png', dpi=300, bbox_inches='tight')
    plt.show()

# ãƒ€ãƒŸãƒ¼é–¢æ•°ï¼ˆä¸è¶³ã—ã¦ã„ã‚‹é–¢æ•°ã®æœ€å°å®Ÿè£…ï¼‰
def plot_combined_temperature_model_comparison(*args, **kwargs):
    print("âš ï¸ plot_combined_temperature_model_comparison ã¯æœªå®Ÿè£…ã§ã™ã€‚")

def plot_model_selection_results_temperature(*args, **kwargs):
    print("âš ï¸ plot_model_selection_results_temperature ã¯æœªå®Ÿè£…ã§ã™ã€‚")

def calculate_temperature_peak_errors(*args, **kwargs):
    print("âš ï¸ calculate_temperature_peak_errors ã¯æœªå®Ÿè£…ã§ã™ã€‚")
    return {}

def save_peak_analysis_to_csv(*args, **kwargs):
    print("âš ï¸ save_peak_analysis_to_csv ã¯æœªå®Ÿè£…ã§ã™ã€‚")
def fit_single_temperature_cavity_modes(dataset: Dict[str, Any]) -> Dict[str, float]:
    """é«˜å‘¨æ³¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ï¼ˆfit_eps_bg_only_temperatureã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
    return fit_eps_bg_only_temperature(dataset)

def save_fitting_parameters_to_csv(final_traces: Dict[str, az.InferenceData], 
                                  temperature_specific_params: Dict[float, Dict[str, float]]):
    """ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    print("\n--- ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°çµæœã‚’CSVã«ä¿å­˜ä¸­ ---")
    
    # ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµæœã‚’ä¿å­˜
    for model_type, trace in final_traces.items():
        params = extract_bayesian_parameters(trace)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿çµæœ
        params_df = pd.DataFrame([params])
        params_file = RESULTS_DIR / f'fitting_parameters_{model_type}.csv'
        params_df.to_csv(params_file, index=False)
        print(f"âœ… ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {params_file}")
    
    # æ¸©åº¦åˆ¥å…‰å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµæœã‚’ä¿å­˜
    temp_params_list = []
    for temp, params in sorted(temperature_specific_params.items()):
        temp_params_list.append(params)
    
    if temp_params_list:
        temp_df = pd.DataFrame(temp_params_list)
        temp_file = RESULTS_DIR / 'temperature_optical_parameters.csv'
        temp_df.to_csv(temp_file, index=False)
        print(f"âœ… æ¸©åº¦åˆ¥å…‰å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜: {temp_file}")

def plot_temperature_dependencies(temperature_specific_params: Dict[float, Dict[str, float]], 
                                trace: az.InferenceData):
    """æ¸©åº¦ä¾å­˜æ€§ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ï¼ˆè†œåšã¯å›ºå®šå€¤ã®ãŸã‚é™¤å¤–ï¼‰"""
    print("\n--- æ¸©åº¦ä¾å­˜æ€§ã®å¯è¦–åŒ– ---")
    
    temperatures = sorted(temperature_specific_params.keys())
    eps_bg_values = [temperature_specific_params[T]['eps_bg'] for T in temperatures]
    
    # ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    magnetic_params = extract_bayesian_parameters(trace)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # eps_bg ã®æ¸©åº¦ä¾å­˜æ€§
    ax1.plot(temperatures, eps_bg_values, 'ro-', linewidth=2, markersize=8, label='èƒŒæ™¯èª˜é›»ç‡')
    ax1.set_xlabel('æ¸©åº¦ (K)', fontsize=12)
    ax1.set_ylabel('èƒŒæ™¯èª˜é›»ç‡ eps_bg', fontsize=12)
    ax1.set_title('èƒŒæ™¯èª˜é›»ç‡ã®æ¸©åº¦ä¾å­˜æ€§', fontsize=14)
    ax1.grid(True, linestyle='--', alpha=0.6)
    ax1.legend()
    
    # å€¤ã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¡¨ç¤º
    for T, eps in zip(temperatures, eps_bg_values):
        ax1.annotate(f'{eps:.2f}', (T, eps), textcoords="offset points", xytext=(0,10), ha='center')
    
    # æ¸©åº¦ã«ã‚ˆã‚‹åŠ¹æœã®æ¦‚è¦ï¼ˆã‚µãƒãƒªãƒ¼ãƒ‘ãƒãƒ«ï¼‰
    ax2.text(0.1, 0.9, f'æ¸©åº¦ç¯„å›²: {min(temperatures)} - {max(temperatures)} K', fontsize=14, transform=ax2.transAxes)
    ax2.text(0.1, 0.8, f'eps_bgå¤‰åŒ–ç‡: {(max(eps_bg_values)-min(eps_bg_values))/min(eps_bg_values)*100:.1f}%', fontsize=14, transform=ax2.transAxes)
    ax2.text(0.1, 0.7, f'å›ºå®šç£å ´: {B_FIXED} T', fontsize=14, transform=ax2.transAxes)
    
    ax2.text(0.1, 0.5, 'ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (æ¸©åº¦éä¾å­˜):', fontsize=14, transform=ax2.transAxes, weight='bold')
    ax2.text(0.1, 0.4, f'gå› å­ = {magnetic_params["g_factor"]:.4f}', fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.3, f'B4 = {magnetic_params["B4"]:.6f} K', fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.2, f'B6 = {magnetic_params["B6"]:.6f} K', fontsize=12, transform=ax2.transAxes)
    ax2.text(0.1, 0.1, f'G0 = {magnetic_params["G0"]:.3e}', fontsize=12, transform=ax2.transAxes)
    
    ax2.set_xlim(0, 1)
    ax2.set_ylim(0, 1)
    ax2.set_title('è§£æçµæœã‚µãƒãƒªãƒ¼', fontsize=14)
    ax2.axis('off')
    
    plt.tight_layout()
    plt.savefig(RESULTS_DIR / 'temperature_dependencies.png', dpi=300, bbox_inches='tight')
    plt.show()

# ãƒ€ãƒŸãƒ¼é–¢æ•°ï¼ˆä¸è¶³ã—ã¦ã„ã‚‹é–¢æ•°ã®æœ€å°å®Ÿè£…ï¼‰
def plot_combined_temperature_model_comparison(*args, **kwargs):
    print("âš ï¸ plot_combined_temperature_model_comparison ã¯æœªå®Ÿè£…ã§ã™ã€‚")

def plot_model_selection_results_temperature(*args, **kwargs):
    print("âš ï¸ plot_model_selection_results_temperature ã¯æœªå®Ÿè£…ã§ã™ã€‚")

def calculate_temperature_peak_errors(*args, **kwargs):
    print("âš ï¸ calculate_temperature_peak_errors ã¯æœªå®Ÿè£…ã§ã™ã€‚")
    return {}

def save_peak_analysis_to_csv(*args, **kwargs):
    print("âš ï¸ save_peak_analysis_to_csv ã¯æœªå®Ÿè£…ã§ã™ã€‚")

def run_analysis_workflow():
    """
    ã€å¤‰æ›´ç‚¹1ã€‘åå¾©ãƒ—ãƒ­ã‚»ã‚¹ã‚’å»ƒæ­¢ã—ã€é‡ã¿ä»˜ã‘ã‚’åˆ©ç”¨ã—ãŸå˜ä¸€ã®è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã€‚
    1. ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€é«˜å‘¨æ³¢ã¨å…¨å‘¨æ³¢æ•°é ˜åŸŸã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã€‚
    2. é«˜å‘¨æ³¢ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„æ¸©åº¦ã®eps_bgã‚’ä¸€åº¦ã ã‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã€‚
    3. å…¨å‘¨æ³¢æ•°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’åŸºã«å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ã‚’ç”Ÿæˆã€‚
    4. é‡ã¿ä»˜ã‘ã—ãŸå°¤åº¦é–¢æ•°ã‚’ç”¨ã„ã¦ãƒ™ã‚¤ã‚ºæ¨å®šã‚’å®Ÿè¡Œã€‚
    5. çµæœã‚’å¯è¦–åŒ–ãƒ»ä¿å­˜ã™ã‚‹ã€‚
    """
    print("ğŸš€ é‡ã¿ä»˜ããƒ™ã‚¤ã‚ºæ¨å®šãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’é–‹å§‹ã—ã¾ã™")
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    all_datasets_full_range = load_data_full_range_temperature(DATA_FILE_PATH, DATA_SHEET_NAME)
    high_freq_datasets = load_and_split_data_three_regions_temperature(
        file_path=DATA_FILE_PATH, sheet_name=DATA_SHEET_NAME
    )['high_freq']

    if not all_datasets_full_range or not high_freq_datasets:
        print("âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
        
    # 2. å„æ¸©åº¦ã®eps_bgã‚’ä¸€åº¦ã ã‘ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    print("\n--- ã‚¹ãƒ†ãƒƒãƒ—1: å„æ¸©åº¦ã®é«˜å‘¨æ³¢eps_bgãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚° ---")
    temperature_specific_params = {}
    for dataset in high_freq_datasets:
        temp = dataset['temperature']
        result = fit_single_temperature_cavity_modes(dataset)
        if result:
            temperature_specific_params[temp] = result
        else:
            # å¤±æ•—ã—ãŸå ´åˆã¯åˆæœŸå€¤ã‚’ä½¿ç”¨
            temperature_specific_params[temp] = {'eps_bg': eps_bg_init, 'd': d_fixed, 'temperature': temp}

    # 3. å‘¨æ³¢æ•°ã”ã¨ã®é‡ã¿é…åˆ—ã‚’ç”Ÿæˆ
    print("\n--- ã‚¹ãƒ†ãƒƒãƒ—2: å°¤åº¦é–¢æ•°ã®ãŸã‚ã®é‡ã¿é…åˆ—ã‚’ç”Ÿæˆ ---")
    weights_list = [create_frequency_weights(d) for d in all_datasets_full_range]

    # 4. é‡ã¿ä»˜ããƒ™ã‚¤ã‚ºæ¨å®šã®å®Ÿè¡Œ (H-form ã¨ B-form)
    final_traces = {}
    prior_params = None
    
    for model_type in ['H_form', 'B_form']:
        print(f"\n--- ã‚¹ãƒ†ãƒƒãƒ—3: {model_type}ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ã‚¤ã‚ºæ¨å®šã‚’å®Ÿè¡Œ ---")
        trace = run_temperature_bayesian_fit(
            all_datasets_full_range,
            temperature_specific_params,
            weights_list,
            prior_magnetic_params=prior_params,
            model_type=model_type
        )
        if trace:
            final_traces[model_type] = trace
            # æœ€åˆã®ãƒ¢ãƒ‡ãƒ«(H_form)ã®çµæœã‚’æ¬¡ã®ãƒ¢ãƒ‡ãƒ«(B_form)ã®äº‹å‰åˆ†å¸ƒã«åˆ©ç”¨
            if prior_params is None:
                prior_params = extract_bayesian_parameters(trace)
            
            # â–¼â–¼â–¼ã€å¤‰æ›´ç‚¹2ã€‘äº‹å¾Œåˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ â–¼â–¼â–¼
            plot_posterior_distributions(trace, model_type)
            # â–²â–²â–²ã€å¤‰æ›´ç‚¹2ã€‘äº‹å¾Œåˆ†å¸ƒã‚’ãƒ—ãƒ­ãƒƒãƒˆ â–²â–²â–²
            
    # 5. çµæœã®è©•ä¾¡ã¨å¯è¦–åŒ–
    if not final_traces:
        print("âŒ ãƒ™ã‚¤ã‚ºæ¨å®šãŒä¸¡ãƒ¢ãƒ‡ãƒ«ã§å¤±æ•—ã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return
        
    print("\n--- ã‚¹ãƒ†ãƒƒãƒ—4: æœ€çµ‚çµæœã®è©•ä¾¡ã¨å¯è¦–åŒ– ---")
    if len(final_traces) >= 2:
        plot_combined_temperature_model_comparison(all_datasets_full_range, temperature_specific_params, final_traces)
        plot_model_selection_results_temperature(final_traces)
        peak_analysis_results = calculate_temperature_peak_errors(all_datasets_full_range, temperature_specific_params, final_traces)
        save_peak_analysis_to_csv(peak_analysis_results)
    
    save_fitting_parameters_to_csv(final_traces, temperature_specific_params)
    plot_temperature_dependencies(temperature_specific_params, list(final_traces.values())[0])

    print("\nğŸ‰ å…¨ã¦ã®è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    # è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
    run_analysis_workflow()