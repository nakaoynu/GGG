# unified_weighted_bayesian_fitting_gpu.py
# GPU (JAX/NumPyro) å®Œå…¨æœ€é©åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³
# ç‰¹å¾´: Pythonãƒ«ãƒ¼ãƒ—ã‚’å»ƒæ­¢ã—ã€å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã§ä¸€æ‹¬å‡¦ç†ã—ã¾ã™
#
# === ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾å¿œå±¥æ­´ ===
# 1. pt.scanã®å®Œå…¨æ’é™¤: 
#    - å…¨ãƒ‡ãƒ¼ã‚¿ã‚’Broadcastingã§ä¸€æ‹¬å‡¦ç†(calculate_susceptibility_vectorized)
#    - å›ºæœ‰å€¤äº‹å‰è¨ˆç®—ã«ã‚ˆã‚Šæ¡ä»¶æ•°ã«æ¯”ä¾‹ã—ãŸè¨ˆç®—é‡ã«å‰Šæ¸›(O(N) â†’ O(n_conditions))
# 2. ç‰©ç†å®šæ•°ã®ã‚¯ãƒ©ã‚¹åŒ–:
#    - PhysicalConstantsã‚¯ãƒ©ã‚¹ã§ã‚¹ã‚³ãƒ¼ãƒ—ã‚’ç®¡ç†ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«æ±šæŸ“ã‚’é˜²æ­¢
# 3. set_subtensoræ’é™¤:
#    - å…¨è¡Œåˆ—ã‚’NumPyã§äº‹å‰æ§‹ç¯‰ã—ã€pt.as_tensor_variableã§å®šæ•°åŒ–
# 4. äº‹å‰åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®configåŒ–:
#    - bayesian_priorsã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ’é™¤
# 5. ä¾‹å¤–å‡¦ç†ã®æ”¹å–„:
#    - bare exceptã‚’Exception catchã«å¤‰æ›´ã€ã‚¨ãƒ©ãƒ¼å‹ã¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ­ã‚°å‡ºåŠ›

import os
import sys
import pathlib
import yaml
import time
import datetime
import warnings
import re

# --- GPUè¨­å®š (ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰ã«å®Ÿè¡Œ) ---
def pre_setup_gpu():
    # config_unified_gpu.yml ã‚’èª­ã‚“ã§GPUè¨­å®šã‚’ç¢ºèª
    config_path = pathlib.Path(__file__).parent / "config_unified_gpu.yml"
    use_gpu = False
    if config_path.exists():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                cfg = yaml.safe_load(f)
            use_gpu = cfg.get('execution', {}).get('use_gpu', False)
        except:
            pass
    
    if use_gpu:
        print("ğŸš€ GPU (JAX) Optimization Enabled")
        # PyTensorã«ã¯float64ä½¿ç”¨ã®ã¿ã‚’ä¼ãˆã€ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã¯JAXã«ä»»ã›ã‚‹
        os.environ['PYTENSOR_FLAGS'] = 'floatX=float64'
        # JAXãŒãƒ¡ãƒ¢ãƒªã‚’ç‹¬å ã—ãªã„ã‚ˆã†ã«ã™ã‚‹è¨­å®š (å…±æœ‰ç’°å¢ƒã§é‡è¦)
        os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'
    else:
        print("ğŸ’» CPU Mode")
        os.environ['PYTENSOR_FLAGS'] = 'device=cpu,floatX=float64'

pre_setup_gpu()

import numpy as np
import pandas as pd
import pymc as pm
import arviz as az
import pytensor.tensor as pt
from pytensor.scan.basic import scan
from pytensor.tensor.slinalg import eigvalsh
from pytensor.tensor.var import TensorVariable
from scipy.signal import find_peaks, peak_widths
from scipy.optimize import curve_fit
from functools import lru_cache
from typing import Any, cast

# ç‰©ç†å®šæ•°ã‚’ã‚¯ãƒ©ã‚¹ã§ç®¡ç†(THzã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰ˆ)
class PhysicalConstants:
    """
    ç‰©ç†å®šæ•°ã®å®šç¾© (THzã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ¸ˆã¿)
    
    ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®åŸç†ã€‘
    è¨ˆç®—ã®æ•°å€¤å®‰å®šæ€§ã‚’ä¿ã¤ãŸã‚ã€å†…éƒ¨è¨ˆç®—ã§ã¯ SIå˜ä½(J, s) ã§ã¯ãªã
    ã€Œ1 THz (= 1e12 Hz) ã®å…‰å­ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã‚’åŸºæº–(=1.0)ã¨ã™ã‚‹å˜ä½ç³»ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
    
    åŸºæœ¬å˜ä½:
      - å‘¨æ³¢æ•°: 1.0 = 1 THz
      - æ™‚é–“:   1.0 = 1 ps (1e-12 s)
      - ã‚¨ãƒãƒ«ã‚®ãƒ¼: 1.0 = h * 1 THz
    
    ã€åŠ¹æœã€‘
    - å¾“æ¥: kB=1.38e-23, hbar=1.05e-34 â†’ å‹¾é…è¨ˆç®—ã§underflow/overflow
    - ä¿®æ­£å¾Œ: kBâ‰ˆ0.021, hbarâ‰ˆ0.159 â†’ å®‰å®šã—ãŸæ•°å€¤è¨ˆç®—
    """
    # --- åŸºæº–ã¨ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ« ---
    SCALE_FREQ = 1.0e12  # 1 THz [Hz]
    
    # --- SIå˜ä½ç³»ã§ã®çœŸå€¤ ---
    _h_SI = 6.62607015e-34    # Planck constant [JÂ·s]
    _hbar_SI = _h_SI / (2 * np.pi)
    _kB_SI = 1.380649e-23     # Boltzmann constant [J/K]
    _muB_SI = 9.274010e-24    # Bohr magneton [J/T]
    
    # --- åŸºæº–ã‚¨ãƒãƒ«ã‚®ãƒ¼: E_base = h * 1THz ---
    _E_base = _h_SI * SCALE_FREQ  # [J]

    # ãƒœãƒ«ãƒ„ãƒãƒ³å®šæ•° [THz/K]
    # æ¸©åº¦T[K]ã‚’æ›ã‘ãŸã¨ãã€kB*T ãŒã€ŒTHzå˜ä½ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã«ãªã‚‹
    kB: float = _kB_SI / _E_base   # â‰ˆ 0.02083 [THz/K]
    
    # ãƒœãƒ¼ã‚¢ç£å­ [THz/T]
    # ç£å ´B[T]ã‚’æ›ã‘ãŸã¨ãã€muB*B ãŒã€ŒTHzå˜ä½ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ã«ãªã‚‹
    muB: float = _muB_SI / _E_base # â‰ˆ 0.01399 [THz/T]
    
    # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯å®šæ•° [ç„¡æ¬¡å…ƒ]
    # E = hbar * omega ã§ã€omegaãŒ[rad/ps]ç›¸å½“ã«ãªã‚‹ã‚ˆã†èª¿æ•´
    # THzå˜ä½ç³»ã§ã¯ hbar = 1/(2Ï€) ãŒæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«
    hbar: float = 1.0 / (2 * np.pi)  # â‰ˆ 0.159
    
    # å…‰é€Ÿ [m/s] (æ³¢é•·è¨ˆç®—ç”¨ã€SIå˜ä½ã®ã¾ã¾)
    c: float = 299792458
    
    # çœŸç©ºã®é€ç£ç‡ [H/m] (SIå˜ä½ã®ã¾ã¾)
    mu0: float = 4.0 * np.pi * 1e-7
    
    # ã‚¹ãƒ”ãƒ³é‡å­æ•°
    s: float = 3.5

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚(æ—¢å­˜ã‚³ãƒ¼ãƒ‰å¯¾å¿œ)
kB = PhysicalConstants.kB
muB = PhysicalConstants.muB
hbar = PhysicalConstants.hbar
c = PhysicalConstants.c
mu0 = PhysicalConstants.mu0
s = PhysicalConstants.s


def _build_sz_matrix() -> np.ndarray:
    return np.diag(np.arange(s, -s - 1, -1, dtype=np.float64))


def _build_o44_matrix() -> np.ndarray:
    mat = np.zeros((8, 8), dtype=np.float64)
    v_s35 = np.sqrt(35.0)
    v_5s3 = 5.0 * np.sqrt(3.0)
    mat[3, 7] = mat[4, 0] = v_s35
    mat[2, 6] = mat[5, 1] = v_5s3
    return 12.0 * (mat + mat.T)


def _build_o64_matrix() -> np.ndarray:
    mat = np.zeros((8, 8), dtype=np.float64)
    v_3s35 = 3.0 * np.sqrt(35.0)
    v_m7s3 = -7.0 * np.sqrt(3.0)
    mat[3, 7] = mat[4, 0] = v_3s35
    mat[2, 6] = mat[5, 1] = v_m7s3
    return 60.0 * (mat + mat.T)


def _build_transition_strength() -> np.ndarray:
    upper_m = np.arange(s, -s, -1, dtype=np.float64)
    return (s + upper_m) * (s - upper_m + 1.0)


SZ_MATRIX_PT = pt.as_tensor_variable(_build_sz_matrix())
O40_MATRIX_PT = pt.as_tensor_variable(60.0 * np.diag([7, -13, -3, 9, 9, -3, -13, 7]))
O44_MATRIX_PT = pt.as_tensor_variable(_build_o44_matrix())
O60_MATRIX_PT = pt.as_tensor_variable(1260.0 * np.diag([1, -5, 9, -5, -5, 9, -5, 1]))
O64_MATRIX_PT = pt.as_tensor_variable(_build_o64_matrix())
TRANSITION_STRENGTH_PT = pt.as_tensor_variable(_build_transition_strength())

PT_KB = pt.as_tensor_variable(np.float64(kB))
PT_MUB = pt.as_tensor_variable(np.float64(muB))
PT_HBAR = pt.as_tensor_variable(np.float64(hbar))
PT_C = pt.as_tensor_variable(np.float64(c))

# =========================================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & å‰å‡¦ç† (æ§‹é€ åŒ–)
# =========================================================

def load_config(path=None):
    if path is None:
        path = pathlib.Path(__file__).parent / "config_unified_gpu.yml"
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def create_results_directory(config: dict) -> pathlib.Path:
    """çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¦è¿”ã™ã€‚

    analysis_results_gpu/run_YYYYmmdd_HHMMSS ã®ã‚ˆã†ãªãƒ‘ã‚¹ã‚’ä½œã‚‹ã€‚
    """
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    parent_dir = config.get('file_paths', {}).get('results_parent_dir', 'analysis_results_gpu')
    results_dir = pathlib.Path(__file__).parent / parent_dir / f"run_{timestamp}"
    results_dir.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨ã—ãŸè¨­å®šã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    try:
        with open(results_dir / "config_used_gpu.yml", 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    except Exception:
        pass

    return results_dir

def create_frequency_weights(freq, trans, config):
    """é‡ã¿é…åˆ—ç”Ÿæˆ (ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¯¾å¿œ)"""
    ws = config['analysis_settings']['weight_settings']
    weights = np.full_like(freq, ws['background_weight'])
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
    peaks, props = find_peaks(trans, 
                              height=ws['peak_height_threshold'], 
                              prominence=ws['peak_prominence_threshold'], 
                              distance=ws['peak_distance'])
    
    # ä½å‘¨æ³¢é ˜åŸŸã«ãƒ”ãƒ¼ã‚¯ãŒã‚ã‚‹ã‹ç¢ºèª
    low_cutoff = config['analysis_settings']['low_freq_cutoff']
    
    if len(peaks) > 0:
        # åŠå€¤å¹…ã‚’è¨ˆç®—ã—ã¦é‡ã¿ä»˜ã‘
        widths, _, lefts, rights = peak_widths(trans, peaks, rel_height=0.5)
        # ãƒ”ãƒ¼ã‚¯ã®floatå‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å‘¨æ³¢æ•°ã«å¤‰æ›
        left_f = np.interp(lefts, np.arange(len(freq)), freq)
        right_f = np.interp(rights, np.arange(len(freq)), freq)
        
        # å…¨ãƒ”ãƒ¼ã‚¯ã«å¯¾ã—ã¦ãƒ«ãƒ¼ãƒ— (ã“ã“ã¯Pythonãƒ«ãƒ¼ãƒ—ã§ã‚‚é«˜é€Ÿ)
        for i in range(len(peaks)):
            mask = (freq >= left_f[i]) & (freq <= right_f[i])
            
            # ä½å‘¨æ³¢ãƒ”ãƒ¼ã‚¯ã‹é«˜å‘¨æ³¢ãƒ”ãƒ¼ã‚¯ã‹ã§é‡ã¿ã‚’å¤‰ãˆã‚‹
            if freq[peaks[i]] < low_cutoff:
                # LP/UP
                weights[mask] = ws['lp_up_peak_weight']
                # ãƒ”ãƒ¼ã‚¯é–“ (ç°¡æ˜“å®Ÿè£…: ãƒ”ãƒ¼ã‚¯å‘¨è¾ºã‚’åºƒã‚ã«å–ã‚‹å‡¦ç†ãŒå¿…è¦ãªã‚‰ã“ã“ã«è¿½åŠ )
            else:
                # é«˜å‘¨æ³¢å…±æŒ¯
                weights[mask] = ws['high_freq_peak_weight']
                
        # ãƒ”ãƒ¼ã‚¯é–“é ˜åŸŸ (LP-UPé–“) ã®é‡ã¿ä»˜ã‘
        # ç°¡æ˜“çš„ã«ã€ä½å‘¨æ³¢ãƒ”ãƒ¼ã‚¯ãŒ2ã¤ä»¥ä¸Šã‚ã‚Œã°ãã®é–“ã‚’åŸ‹ã‚ã‚‹
        low_peaks_idx = np.where(freq[peaks] < low_cutoff)[0]
        if len(low_peaks_idx) >= 2:
            min_p = np.min(freq[peaks[low_peaks_idx]])
            max_p = np.max(freq[peaks[low_peaks_idx]])
            between_mask = (freq >= min_p) & (freq <= max_p) & (weights == ws['background_weight'])
            weights[between_mask] = ws['between_peaks_weight']

    return weights

def load_and_prepare_data(config):
    """
    GPUè¨ˆç®—ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦èª­ã¿è¾¼ã‚€ã€‚
    ä¿®æ­£ç‚¹: èƒŒæ™¯èª˜é›»ç‡ eps_bg ã‚’ (æ¸©åº¦, ç£å ´) ã®æ¡ä»¶ã”ã¨ã«ç®¡ç†ã™ã‚‹ãŸã‚ã€
    æ¡ä»¶ID condition_id ã¨ãƒ¦ãƒ‹ãƒ¼ã‚¯æ¡ä»¶ãƒªã‚¹ãƒˆ unique_conditions ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    """
    print("\n--- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨GPUç”¨æ§‹é€ åŒ– ---")

    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è§£æ±º
    input_files = config['file_paths'].get('input_files', [])
    if not input_files:
        input_files = [{
            'file_path': config['file_paths']['data_file'],
            'type': 'temperature', 'fixed_param': 9.0
        }]

    # 2. ä¸€æ™‚ãƒªã‚¹ãƒˆã«åé›†
    all_freq: list[np.ndarray] = []
    all_trans: list[np.ndarray] = []
    all_temp: list[np.ndarray] = []
    all_bfield: list[np.ndarray] = []
    all_weights: list[np.ndarray] = []

    # æ¡ä»¶ (T,B) ã‚’ä¸€æ„é›†åˆã§ç®¡ç†
    unique_conditions: set[tuple[float, float]] = set()

    count = 0
    for file_info in input_files:
        # ãƒ‘ã‚¹è§£æ±º (Linux/Windowså¯¾å¿œ)
        raw_path = file_info['file_path'].replace('\\', '/')
        path = pathlib.Path(raw_path)
        if not path.exists():
            path = pathlib.Path(__file__).parent / raw_path

        if not path.exists():
            print(f"âš ï¸ Skip: {raw_path}")
            continue

        try:
            df = pd.read_excel(path, sheet_name=config['file_paths']['sheet_name'])
            freq_col = df.columns[0]
            freq_vals = df[freq_col].to_numpy()

            for col in df.columns[1:]:
                col_str = str(col)
                if not any(char.isdigit() for char in col_str):
                    continue

                m = re.search(r"([\d\.]+)", col_str)
                if not m:
                    continue
                val = float(m.group(1))

                if file_info['type'] == 'temperature':
                    t_val = val
                    b_val = float(file_info['fixed_param'])
                else:
                    t_val = float(file_info['fixed_param'])
                    b_val = val

                trans_vals = df[col].values
                mask = np.isfinite(trans_vals)
                f_clean = freq_vals[mask]
                t_clean = trans_vals[mask]
                if len(f_clean) == 0:
                    continue

                w_clean = create_frequency_weights(f_clean, t_clean, config)

                all_freq.append(f_clean)
                all_trans.append(t_clean)
                all_weights.append(w_clean)
                all_temp.append(np.full_like(f_clean, t_val))
                all_bfield.append(np.full_like(f_clean, b_val))

                unique_conditions.add((float(t_val), float(b_val)))
                count += 1

        except Exception as e:
            print(f"âŒ Error reading {path.name}: {e}")

    if count == 0:
        raise RuntimeError("æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")

    # 3. Flatten
    flat_freq = np.concatenate(all_freq)
    flat_trans = np.concatenate(all_trans)
    flat_weights = np.concatenate(all_weights)
    flat_temp = np.concatenate(all_temp)
    flat_bfield = np.concatenate(all_bfield)

    # 4. æ¡ä»¶IDã®ç”Ÿæˆ (æ¸©åº¦æ˜‡é †â†’ç£å ´æ˜‡é †)
    sorted_conditions = sorted(list(unique_conditions), key=lambda x: (x[0], x[1]))
    cond_to_id = {cond: i for i, cond in enumerate(sorted_conditions)}
    flat_cond_ids = np.array([cond_to_id[(t, b)] for t, b in zip(flat_temp, flat_bfield)], dtype=np.int32)

    print(f"âœ… ãƒ‡ãƒ¼ã‚¿çµåˆå®Œäº†: {len(flat_freq)} ç‚¹, {len(sorted_conditions)} æ¡ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ")

    return {
        'freq': flat_freq,  # THzå˜ä½
        # ã€é‡è¦ä¿®æ­£ã€‘THzã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: omega = freq * 2Ï€ (1e12ã‚’æ›ã‘ãªã„)
        # ç‰©ç†å®šæ•°å´ã§ 1.0 = 1THz ã¨å®šç¾©ã—ãŸãŸã‚ã€è§’å‘¨æ³¢æ•°ã‚‚åŒã˜ã‚¹ã‚±ãƒ¼ãƒ«
        'omega': flat_freq * 2 * np.pi,  # [THz] â†’ [rad/ps]ç›¸å½“
        'trans': flat_trans,
        'weights': flat_weights,
        'temp': flat_temp,
        'b_field': flat_bfield,
        'condition_id': flat_cond_ids,
        'unique_conditions': sorted_conditions,
        'original_datasets_count': count,
    }

# =========================================================
# 2. NumPyç‰ˆ ç‰©ç†é–¢æ•° (Step 1ç”¨)
# =========================================================
# â€»ã“ã“ã¯ scipy.optimize ç”¨ãªã®ã§ NumPy ã®ã¾ã¾ã§OK
from unified_weighted_bayesian_fitting import calculate_susceptibility as calculate_susceptibility_numpy
from unified_weighted_bayesian_fitting import normalize_gamma_array
@lru_cache(maxsize=1024)
def _get_hamiltonian_numpy_cached(B, g, B4, B6):
    """
    get_hamiltonian ã®çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹å†…éƒ¨é–¢æ•°
    å¼•æ•°ã¯ãƒãƒƒã‚·ãƒ¥å¯èƒ½ï¼ˆfloatç­‰ï¼‰ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹
    """
    from unified_weighted_bayesian_fitting import get_hamiltonian
    return get_hamiltonian(B, g, B4, B6)

def get_hamiltonian_numpy(B, g, B4, B6):
    """
    Step 1ç”¨ã®ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³å–å¾—é–¢æ•°ã€‚
    å…ƒã®CPUç‰ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®åŸºæœ¬é–¢æ•°ã‚’åˆ©ç”¨ã—ã¤ã¤ã€ã“ã“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¡Œã£ã¦é«˜é€ŸåŒ–ã™ã‚‹ã€‚
    """
    # lru_cache ã‚’åˆ©ç”¨ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    B_r = round(float(B), 6)
    g_r = round(float(g), 6)
    B4_r = round(float(B4), 8)
    B6_r = round(float(B6), 8)
    
    return _get_hamiltonian_numpy_cached(B_r, g_r, B4_r, B6_r)

def calculate_transmission_numpy(omega, mu_r, d, eps_bg):
    from unified_weighted_bayesian_fitting import calculate_normalized_transmission
    return calculate_normalized_transmission(omega, mu_r, d, eps_bg)

from unified_weighted_bayesian_fitting import get_eps_bg_initial_values_and_bounds  # åˆæœŸå€¤ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’å†åˆ©ç”¨
def fit_eps_bg_step1(unified_data, current_params, config):
    """
    Step 1: é«˜å‘¨æ³¢é ˜åŸŸã§ã® eps_bg æœ€é©åŒ–ï¼ˆæ¡ä»¶ (T, B) ã”ã¨ã«ç‹¬ç«‹æ¨å®šï¼‰
    """
    print("\n--- Step 1: å…‰å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (eps_bg) ã®æœ€é©åŒ– ---")

    high_cutoff = config['analysis_settings']['high_freq_cutoff']
    d_fixed = config['physical_parameters']['d_fixed']
    N_spin = config['physical_parameters']['N_spin']

    # æ¡ä»¶ãƒªã‚¹ãƒˆï¼ˆ(temp, b_field)ï¼‰
    unique_conditions: list[tuple[float, float]] = unified_data['unique_conditions']

    # çµæœæ ¼ç´ç”¨è¾æ›¸ {(t, b): eps_bg_value}
    eps_bg_map: dict[tuple[float, float], float] = {}

    # ç¾åœ¨ã®ç£æ°—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    g = current_params['g_factor']
    B4 = current_params['B4']
    B6 = current_params['B6']
    a_scale = current_params['a_scale']

    # gamma ã¯ç°¡æ˜“ä¸€å®šï¼ˆ7é·ç§»åˆ†ï¼‰
    gamma_val = current_params.get('gamma_mean', 0.11e12)
    gamma_arr = np.full(7, gamma_val)

    # å®šæ•°
    G0 = a_scale * mu0 * N_spin * (g * muB)**2 / (2 * hbar)

    for (temp, b_val) in unique_conditions:
        # æ¡ä»¶ä¸€è‡´ã‹ã¤é«˜å‘¨æ³¢åŸŸ
        mask = (
            (unified_data['temp'] == temp) &
            (unified_data['b_field'] == b_val) &
            (unified_data['freq'] >= high_cutoff)
        )

        key = (temp, b_val)

        if np.sum(mask) < 5:
            eps_bg_map[key] = 14.2
            continue

        freq_sample = unified_data['freq'][mask]
        omega_sample = unified_data['omega'][mask]
        trans_sample = unified_data['trans'][mask]

        # å›ºå®šç£å ´ b_val ã§ã®ç‰©ç†ãƒ¢ãƒ‡ãƒ«
        H = get_hamiltonian_numpy(b_val, g, B4, B6)
        chi = calculate_susceptibility_numpy(omega_sample, H, temp, gamma_arr)
        mu_r_base = 1.0 + G0 * chi  # H_form è¿‘ä¼¼

        def fit_func(f_dummy, eps_val):
            return calculate_transmission_numpy(omega_sample, mu_r_base, d_fixed, eps_val)

        # CPUç‰ˆã®å …ç‰¢ãƒ­ã‚¸ãƒƒã‚¯ã«åˆã‚ã›ãŸåˆæœŸå€¤ã‚¹ã‚¤ãƒ¼ãƒ—
        initial_eps_bg_values, bounds_eps_bg = get_eps_bg_initial_values_and_bounds(temp)
        success = False
        best_eps = 14.2
        min_err = float('inf')

        for initial_eps in initial_eps_bg_values:
            try:
                popt, _ = curve_fit(
                    fit_func,
                    freq_sample,
                    trans_sample,
                    p0=[initial_eps],
                    bounds=([bounds_eps_bg[0]], [bounds_eps_bg[1]]),
                    maxfev=3000,
                )
                eps_fit = float(popt[0])
                if bounds_eps_bg[0] <= eps_fit <= bounds_eps_bg[1]:
                    pred = fit_func(None, eps_fit)
                    err = float(np.sum((trans_sample - pred) ** 2))
                    if err < min_err:
                        min_err = err
                        best_eps = eps_fit
                        success = True
            except Exception as e:
                # curve_fitã®åæŸå¤±æ•—ã‚„ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
                print(f"    Debug: curve_fit failed with initial_eps={initial_eps:.2f}: {type(e).__name__}")
                continue

        if success:
            eps_bg_map[key] = best_eps
        else:
            print(f"  âš ï¸ Fit failed: T={temp}K, B={b_val}T -> Default used")
            eps_bg_map[key] = 14.2

    print(f"âœ… {len(eps_bg_map)} æ¡ä»¶ã® eps_bg ã‚’ç‹¬ç«‹ã«æ±ºå®šã—ã¾ã—ãŸã€‚")
    return eps_bg_map

# =========================================================
# 3. PyTensorç‰ˆ ç‰©ç†é–¢æ•° (Step 2: MCMCç”¨)
# =========================================================

def get_hamiltonian_pt_impl(B_ext_z, g_factor, B4, B6):
    """Return the 8x8 Hamiltonian tensor for a given field."""
    B_ext_z_pt = pt.as_tensor_variable(B_ext_z)
    g_factor_pt = pt.as_tensor_variable(g_factor)
    B4_pt = pt.as_tensor_variable(B4)
    B6_pt = pt.as_tensor_variable(B6)

    crystal_field = (B4_pt * PT_KB) * (O40_MATRIX_PT + 5.0 * O44_MATRIX_PT)
    crystal_field += (B6_pt * PT_KB) * (O60_MATRIX_PT - 21.0 * O64_MATRIX_PT)
    zeeman = g_factor_pt * PT_MUB * B_ext_z_pt * SZ_MATRIX_PT
    return crystal_field + zeeman


def precompute_eigenvalues_for_conditions(unique_conditions, g_factor, B4, B6):
    """äº‹å‰ã«å„æ¡ä»¶ã®å›ºæœ‰å€¤ã‚’è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŒ–ï¼ˆæ”¹å–„1ï¼‰"""
    eigvals_list = []
    for (temp, b_field) in unique_conditions:
        H = get_hamiltonian_pt_impl(b_field, g_factor, B4, B6)
        eigvals = eigvalsh(H, b=None)  # type: ignore[call-arg]
        eigvals_shifted = eigvals - pt.min(eigvals)
        eigvals_list.append(eigvals_shifted)
    
    # Stack to create lookup tensor: shape (n_conditions, 8)
    eigvals_tensor = pt.stack(eigvals_list)
    return eigvals_tensor


def calculate_susceptibility_vectorized(omega_array, temp_array, condition_id, eigvals_cache, gamma_array):
    """Vectorizedç£åŒ–ç‡è¨ˆç®—(æ”¹å–„1,2,æ¸©åº¦ä¾å­˜gammaå¯¾å¿œ)
    
    Args:
        omega_array: è§’å‘¨æ³¢æ•°é…åˆ— shape (N,)
        temp_array: æ¸©åº¦é…åˆ— shape (N,)
        condition_id: æ¡ä»¶IDé…åˆ— shape (N,)
        eigvals_cache: å›ºæœ‰å€¤ãƒ†ãƒ³ã‚½ãƒ« shape (n_conditions, 8)
        gamma_array: ç·©å’Œæ™‚é–“é…åˆ— shape (N, 7) ã¾ãŸã¯ (7,)
    Returns:
        chi: ç£åŒ–ç‡é…åˆ— shape (N,)
    """
    # æ¡ä»¶IDã‹ã‚‰å›ºæœ‰å€¤ã‚’å–å¾—: shape (N, 8)
    eigvals = eigvals_cache[condition_id]
    
    # æ¸©åº¦ã”ã¨ã«Boltzmannåˆ†å¸ƒã‚’è¨ˆç®—: shape (N, 8)
    beta = 1.0 / (PT_KB * temp_array[:, None])  # (N, 1)
    weights = pt.exp(-eigvals * beta)  # (N, 8)
    partition = pt.sum(weights, axis=1, keepdims=True)  # (N, 1)
    pops = weights / partition  # type: ignore[operator]  # (N, 8)
    
    # ã‚¨ãƒãƒ«ã‚®ãƒ¼å·®ã¨é›†å›£å·®: shape (N, 7)
    delta_e = eigvals[:, 1:] - eigvals[:, :-1]  # (N, 7)
    delta_pop = pops[:, 1:] - pops[:, :-1]  # (N, 7)
    omega_0 = delta_e / PT_HBAR  # (N, 7)
    
    # é·ç§»å¼·åº¦ã‚’æ›ã‘ã‚‹: shape (N, 7)
    numer = delta_pop * TRANSITION_STRENGTH_PT[None, :]  # (N, 7)
    
    # ç£åŒ–ç‡è¨ˆç®—(å‘¨æ³¢æ•°ä¾å­˜)
    # omega_array: (N,), omega_0: (N, 7), gamma_array: (N, 7) ã¾ãŸã¯ (7,)
    # gamma_arrayãŒ(7,)ã®å ´åˆã¯[None, :]ã§ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã€(N,7)ãªã‚‰ãã®ã¾ã¾ä½¿ç”¨
    gamma_broadcast = gamma_array if gamma_array.ndim == 2 else gamma_array[None, :]
    denom = omega_0 - omega_array[:, None] - 1j * gamma_broadcast  # (N, 7)
    chi = pt.sum(numer / denom, axis=1)  # (N,)
    
    return -chi  # type: ignore[operator]


def calculate_transmission_vectorized(omega_array, mu_r_array, d, eps_bg_array):
    """Vectorizedé€éç‡è¨ˆç®—(æ”¹å–„2)
    
    Args:
        omega_array: è§’å‘¨æ³¢æ•°é…åˆ— shape (N,)
        mu_r_array: é€ç£ç‡é…åˆ— shape (N,)
        d: ã‚µãƒ³ãƒ—ãƒ«åš(ã‚¹ã‚«ãƒ©ãƒ¼)
        eps_bg_array: èƒŒæ™¯èª˜é›»ç‡é…åˆ— shape (N,)
    Returns:
        transmission: é€éç‡é…åˆ— shape (N,)
    """
    # å…¨ã¦ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦å‡¦ç†
    mu_r_c = pt.cast(mu_r_array, "complex128")
    eps_bg_c = pt.cast(eps_bg_array, "complex128")
    
    # eps * mu
    eps_mu = eps_bg_c * mu_r_c
    real_eps = pt.real(eps_mu)
    imag_eps = pt.imag(eps_mu)
    
    # å®Ÿéƒ¨ãŒè² ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    fallback = pt.cast(0.1, "complex128") + 1j * imag_eps  # type: ignore[operator]
    condition = pt.gt(real_eps, 0.0)
    safe_eps_mu = pt.switch(condition, eps_mu, fallback)
    
    # å±ˆæŠ˜ç‡ã¨ã‚¤ãƒ³ãƒ”ãƒ¼ãƒ€ãƒ³ã‚¹
    n = pt.sqrt(safe_eps_mu)  # type: ignore[operator]
    impedance = pt.sqrt(mu_r_c / eps_bg_c)  # type: ignore[operator]
    
    # ä½ç›¸å·®
    wavelength = (2.0 * np.pi * PT_C) / omega_array
    delta = (2.0 * np.pi * n * d) / wavelength  # type: ignore[operator]
    
    # é€éä¿‚æ•°
    numerator = 4.0 * impedance  # type: ignore[operator]
    term_pos = (1.0 + impedance) ** 2 * pt.exp(-1j * delta)  # type: ignore[operator]
    term_neg = (1.0 - impedance) ** 2 * pt.exp(1j * delta)  # type: ignore[operator]
    transmission = numerator / (term_pos - term_neg)
    
    return pt.abs(transmission) ** 2  # type: ignore[operator]

# =========================================================
# 4. ãƒ¡ã‚¤ãƒ³ MCMC ãƒ«ãƒ¼ãƒãƒ³
# =========================================================

def run_mcmc_gpu_optimized(unified_data, eps_bg_map, config, model_type):
    print(f"\nğŸš€ {model_type}: GPU Vectorized MCMC Start")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’PyTensorå®šæ•°ã«å¤‰æ›
    omega_pt = pt.as_tensor_variable(unified_data['omega'])
    trans_pt = pt.as_tensor_variable(unified_data['trans'])
    temp_pt = pt.as_tensor_variable(unified_data['temp'])
    b_pt = pt.as_tensor_variable(unified_data['b_field'])
    weights_pt = pt.as_tensor_variable(unified_data['weights'])
    
    # æ¡ä»¶IDã§ eps_bg ã‚’ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—
    sorted_conditions = unified_data['unique_conditions']  # [(t, b), ...]
    eps_values = np.array([eps_bg_map[cond] for cond in sorted_conditions])
    eps_lookup = pt.as_tensor_variable(eps_values)

    cond_id_pt = pt.as_tensor_variable(unified_data['condition_id'])
    eps_bg_pt = eps_lookup[cond_id_pt]
    
    d_fixed = config['physical_parameters']['d_fixed']
    N_spin = config['physical_parameters']['N_spin']
    
    # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š(æ”¹å–„3)
    total_points = len(unified_data['omega'])
    batch_size = config.get('mcmc', {}).get('batch_size', min(10000, total_points))
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {total_points}, ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    
    # äº‹å‰åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’configã‹ã‚‰èª­ã¿è¾¼ã¿(ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ’é™¤)
    priors_cfg = config.get('bayesian_priors', {})
    mag_priors = priors_cfg.get('magnetic_parameters', {})
    gamma_priors = priors_cfg.get('gamma_parameters', {})
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤(configæœªè¨­å®šæ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)
    g_mu = mag_priors.get('g_factor', {}).get('mu', 2.0)
    g_sigma = mag_priors.get('g_factor', {}).get('sigma', 0.1)
    B4_mu = mag_priors.get('B4', {}).get('mu', 0.0005)
    B4_sigma = mag_priors.get('B4', {}).get('sigma', 0.0001)
    B6_mu = mag_priors.get('B6', {}).get('mu', 0.00005)
    B6_sigma = mag_priors.get('B6', {}).get('sigma', 0.00001)
    a_scale_sigma = mag_priors.get('a_scale', {}).get('sigma', 1.0)
    
    log_gamma_mu_val = gamma_priors.get('log_gamma_mu_base', {}).get('mu', 25.0)
    log_gamma_sigma_val = gamma_priors.get('log_gamma_mu_base', {}).get('sigma', 1.0)
    temp_slope_sigma = gamma_priors.get('temp_gamma_slope', {}).get('sigma', 0.01)
    log_sigma_val = gamma_priors.get('log_gamma_sigma_base', {}).get('sigma', 0.3)
    log_offset_sigma = gamma_priors.get('log_gamma_offset_base', {}).get('sigma', 0.3)
    
    with pm.Model() as model:
        # --- Parameters (configé§†å‹•) ---
        a_scale = pm.HalfNormal('a_scale', sigma=a_scale_sigma)
        g_factor = pm.Normal('g_factor', mu=g_mu, sigma=g_sigma)
        B4 = pm.Normal('B4', mu=B4_mu, sigma=B4_sigma)
        B6 = pm.Normal('B6', mu=B6_mu, sigma=B6_sigma)
        
        log_gamma_mu = pm.Normal('log_gamma_mu_base', mu=log_gamma_mu_val, sigma=log_gamma_sigma_val)
        temp_slope = pm.Normal('temp_gamma_slope', mu=0.0, sigma=temp_slope_sigma)
        log_sigma = pm.HalfNormal('log_gamma_sigma_base', sigma=log_sigma_val)
        log_offset = pm.Normal('log_gamma_offset_base', mu=0.0, sigma=log_offset_sigma, shape=7)
        
        # --- å›ºæœ‰å€¤äº‹å‰è¨ˆç®—(æ”¹å–„1)---
        eigvals_cache = precompute_eigenvalues_for_conditions(
            sorted_conditions, g_factor, B4, B6
        )
        
        # --- Gammaé…åˆ—è¨ˆç®—(æ”¹å–„4: é·ç§»ã”ã¨ç‹¬ç«‹ + æ¸©åº¦ä¾å­˜æ€§ä¿®æ­£)---
        # 1. é·ç§»ã”ã¨ã®ã‚ªãƒ•ã‚»ãƒƒãƒˆé … (æ¸©åº¦ã«ä¾å­˜ã—ãªã„å½¢çŠ¶å› å­)
        # shape: (1, 7) ã«ã—ã¦ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆæº–å‚™
        gamma_offsets = pt.exp(log_offset * log_sigma)[None, :]  # type: ignore[index]  # (1, 7)
        
        # 2. æ¸©åº¦ä¾å­˜é … (ãƒ‡ãƒ¼ã‚¿ç‚¹ã”ã¨ã«è¨ˆç®—)
        # temp_pt: (N,) -> (N, 1) ã«å¤‰å½¢
        temp_diff = (temp_pt - 4.0)[:, None]  # type: ignore[index]  # (N, 1)
        
        # log_gamma_mu + slope * diff -> (N, 1)
        log_gamma_base_vec = log_gamma_mu + temp_slope * temp_diff
        gamma_base_vec = pt.exp(log_gamma_base_vec)  # (N, 1)
        
        # 3. çµ±åˆ (N, 1) * (1, 7) -> (N, 7)
        # å„ãƒ‡ãƒ¼ã‚¿ç‚¹ iã€å„é·ç§» j ã«å¯¾å¿œã™ã‚‹ gamma[i, j]
        gamma_unified = gamma_base_vec * gamma_offsets  # type: ignore[operator]  # shape: (N, 7)
        
        # --- Vectorizedè¨ˆç®—(æ”¹å–„2: scanå®Œå…¨æ’é™¤)---
        # ç£åŒ–ç‡è¨ˆç®—
        chi = calculate_susceptibility_vectorized(
            omega_pt, temp_pt, cond_id_pt, eigvals_cache, gamma_unified
        )
        
        # G0å®šæ•°
        G0 = a_scale * mu0 * N_spin * (g_factor * muB)**2 / (2 * hbar)
        
        # é€ç£ç‡è¨ˆç®—
        if model_type == 'B_form':
            mu_r = 1.0 / (1.0 - G0 * chi)
        else:
            mu_r = 1.0 + G0 * chi
        
        # é€éç‡è¨ˆç®—(Vectorized)
        mu_pred = calculate_transmission_vectorized(
            omega_pt, mu_r, d_fixed, eps_bg_pt
        )
        
        # --- Likelihood ---
        # é‡ã¿ä»˜ããƒã‚¤ã‚º
        sigma_base = pm.HalfNormal('sigma_base', sigma=0.05)
        # w < 1e-6 ã®å ´æ‰€ã¯ç„¡è¦–ã™ã‚‹ãŸã‚ã€sigmaã‚’å¤§ããã™ã‚‹
        w_safe = pt.clip(weights_pt, 1e-6, 1e10)
        sigma_vec = sigma_base / pt.sqrt(w_safe)
        
        pm.Normal('obs', mu=mu_pred, sigma=sigma_vec, observed=trans_pt)
        
        # --- Sampling ---
        trace = pm.sample(
            draws=config['mcmc']['draws'],
            tune=config['mcmc']['tune'],
            chains=config['mcmc']['chains'],
            nuts_sampler="numpyro",
            random_seed=42
        )
        return trace

# =========================================================
# 5. Main Loop
# =========================================================
def main():
    config = load_config()
    results_dir = create_results_directory(config)
    
    # 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ (ä¸€æ‹¬)
    unified_data = load_and_prepare_data(config)
    
    # åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    current_params = {
        'g_factor': 2.0, 'B4': 0.0005, 'B6': 0.00005, 'a_scale': 1.0, 'gamma_mean': 0.11e12
    }
    
    for i in range(config['mcmc']['max_iterations']):
        print(f"\n=== Iteration {i+1} ===")
        
        # Step 1: eps_bg æœ€é©åŒ–
        # NumPyã§è¨ˆç®—ã™ã‚‹ã®ã§é«˜é€Ÿã€ã‹ã¤é«˜å‘¨æ³¢ã®ã¿ãªã®ã§ç°¡å˜
        eps_bg_map = fit_eps_bg_step1(unified_data, current_params, config)
        
        # Step 2: MCMC
        trace = run_mcmc_gpu_optimized(unified_data, eps_bg_map, config, 'H_form')
        
        # çµæœä¿å­˜
        az.to_netcdf(trace, results_dir / f"trace_iter{i}.nc")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–° (type: ignore for xarray typing)
        summary_result = az.summary(trace)
        g_val = summary_result.loc['g_factor', 'mean']  # type: ignore[index]
        b4_val = summary_result.loc['B4', 'mean']  # type: ignore[index]
        current_params['g_factor'] = float(g_val.item() if hasattr(g_val, 'item') else g_val)  # type: ignore[arg-type]
        current_params['B4'] = float(b4_val.item() if hasattr(b4_val, 'item') else b4_val)  # type: ignore[arg-type]
        # ... (ä»–ã‚‚æ›´æ–°) ...
        print("Updated Params:", current_params)

if __name__ == "__main__":
    main()
